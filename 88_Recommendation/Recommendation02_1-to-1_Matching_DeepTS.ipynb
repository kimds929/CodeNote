{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58694f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b488e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "# rng = np.random.default_rng(0)\n",
    "rng = np.random.RandomState(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50808e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v, eps=1e-9):\n",
    "    n = np.linalg.norm(v) + eps\n",
    "    return v / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfa5ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bianry_to_decimal = lambda binary_list: int(''.join(map(str, binary_list)), 2)\n",
    "\n",
    "user_dim = 5\n",
    "user_vec = (rng.rand(user_dim) > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9e82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "class AgentTS:\n",
    "    id = -1\n",
    "    true_compatibility_dim = rng.randint(20,50)\n",
    "    \n",
    "    def __init__(self, user_dim=4, compatibility_dim=16, lambda0=1, noise_sigma=0.1, forgetting_decay=1, sparsity = 0.2):\n",
    "        # uhknown true compatibility vector\n",
    "        self.user_dim = user_dim\n",
    "        \n",
    "        \n",
    "        # True Compativility ---------------------------------------------------------------------\n",
    "        # raw = rng.normal(size=AgentTS.true_compatibility_dim)\n",
    "        raw = rng.uniform(low=-0.5, high=0.5, size=AgentTS.true_compatibility_dim)\n",
    "        mask = (rng.rand(AgentTS.true_compatibility_dim) < sparsity).astype(float)\n",
    "        raw *= mask\n",
    "\n",
    "        # 모든 차원이 0이 되는 극단 케이스는 피하기 위해 한 번 더 체크\n",
    "        if np.all(raw == 0):\n",
    "            # 최소 한 개는 활성화\n",
    "            idx = rng.randint(AgentTS.true_compatibility_dim)\n",
    "            raw[idx] = rng.normal()\n",
    "        self.true_compatibility = normalize(raw)\n",
    "        # self.true_compatibility = normalize(\n",
    "        #     rng.uniform(low=-1, high=1, size=AgentTS.true_compatibility_dim)\n",
    "        #     )\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        \n",
    "        self.user_vec = (rng.rand(user_dim) > 0.5).astype(np.int64)\n",
    "        self.user_vec_group = int(''.join(map(str, self.user_vec)), 2)\n",
    "        \n",
    "        AgentTS.id += 1\n",
    "        self.id = AgentTS.id\n",
    "        \n",
    "        # info\n",
    "        self.compatibility_dim = compatibility_dim\n",
    "        self.noise_sigma = noise_sigma\n",
    "        self.lambda0 = lambda0      # λ_0·I : Ridge Reguarization parameters (얼마나 prior를 신뢰할지?)\n",
    "        self.forgetting_decay = forgetting_decay\n",
    "        \n",
    "        self.reset_params()\n",
    "        # self.A = lambda0 * np.eye(compatibility_dim)\n",
    "        # self.b = np.zeros(compatibility_dim)\n",
    "        \n",
    "        # self.Sigma = np.linalg.inv(self.A)      # 현재 추정된 compatibility Covariance\n",
    "        # self.mu = rng.uniform(size=compatibility_dim)*0.1\n",
    "    \n",
    "    def reset_params(self):\n",
    "        self.A = self.lambda0 * np.eye(self.compatibility_dim)\n",
    "        self.b = np.zeros(self.compatibility_dim)\n",
    "        \n",
    "        self.Sigma = np.linalg.inv(self.A)      # 현재 추정된 compatibility Covariance\n",
    "        self.mu = rng.uniform(size=self.compatibility_dim)*0.1\n",
    "    \n",
    "    def update(self, compatibility_other, r, Sigma_other=None, inplace=True):\n",
    "        x_compatibility = compatibility_other.copy()\n",
    "        A = self.forgetting_decay * self.A\n",
    "        b = self.forgetting_decay * self.b\n",
    "        \n",
    "        E_xx = np.outer(x_compatibility, x_compatibility)\n",
    "        if Sigma_other is not None:\n",
    "            E_xx += Sigma_other\n",
    "        A += (1/ self.noise_sigma) * E_xx    # += 1/(σ^2) * (X_I^T X_I)\n",
    "        b += (1/ self.noise_sigma) * x_compatibility * r        # +=  1/(σ^2) * (X_I r)\n",
    "        \n",
    "        Sigma = np.linalg.inv(A)\n",
    "        mu = Sigma @ b\n",
    "        \n",
    "        if inplace is True:\n",
    "            self.A = A\n",
    "            self.b = b\n",
    "            self.Sigma = Sigma\n",
    "            self.mu = mu\n",
    "        \n",
    "        return mu, Sigma\n",
    "    \n",
    "    def sampling(self):\n",
    "        return rng.multivariate_normal(self.mu, self.Sigma)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"UserAgent_{self.id}\"\n",
    "\n",
    "# COMPATIBILITY_DIM = 16\n",
    "# users = [AgentTS(user_dim=4, compatibility_dim=COMPATIBILITY_DIM) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a86520",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "class TrueCompatibilityNet(nn.Module):\n",
    "    def __init__(self, compatibility_dim):\n",
    "        super().__init__()\n",
    "        self.compatibility_dim = compatibility_dim\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(compatibility_dim, compatibility_dim*2)\n",
    "            ,nn.ReLU()\n",
    "            ,nn.Linear(compatibility_dim*2, compatibility_dim*4)\n",
    "            ,nn.ReLU()\n",
    "            ,nn.Linear(compatibility_dim*4, compatibility_dim*2)\n",
    "            ,nn.ReLU()\n",
    "            ,nn.Linear(compatibility_dim*2, compatibility_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(torch.FloatTensor(x)).detach().to('cpu').numpy()\n",
    "\n",
    "# true_net = TrueCompatibilityNet(AgentTS.true_compatibility_dim)\n",
    "# true_net(users[0].true_compatibility)\n",
    "# true_net( np.stack([user.true_compatibility for user in users]) ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f131401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# ---------- Helpers: upper-tri vectorization ----------\n",
    "def triu_indices(k, device=None, dtype=torch.long):\n",
    "    return torch.triu_indices(k, k, offset=0, device=device, dtype=dtype)\n",
    "\n",
    "def vec_to_triu(vec, k, device=None):\n",
    "    \"\"\"\n",
    "    vec: (..., k*(k+1)//2)\n",
    "    return: (..., k, k) upper-triangular matrix with zeros elsewhere\n",
    "    \"\"\"\n",
    "    idx = triu_indices(k, device=vec.device)\n",
    "    out = vec.new_zeros(*vec.shape[:-1], k, k)\n",
    "    out[..., idx[0], idx[1]] = vec\n",
    "    return out\n",
    "\n",
    "def triu_to_vec(U):\n",
    "    \"\"\"\n",
    "    U: (..., k, k) upper-triangular\n",
    "    return: (..., k*(k+1)//2)\n",
    "    \"\"\"\n",
    "    k = U.shape[-1]\n",
    "    idx = triu_indices(k, device=U.device)\n",
    "    return U[..., idx[0], idx[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58f377f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "\n",
    "class CategorialEmbedding(nn.Module):\n",
    "    def __init__(self, n_features, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.nf, self.ne, self.ed = n_features, num_embeddings, embedding_dim\n",
    "        self.embedding = nn.Embedding(n_features * num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., n_features), long\n",
    "        *batch, F = x.shape\n",
    "        device = x.device\n",
    "        feature_idx = torch.arange(F, device=device).view(*([1]*len(batch)), F).expand(*x.shape)\n",
    "        flat_idx = feature_idx * self.ne + x                        # (..., F)\n",
    "        out = self.embedding(flat_idx)                                      # (..., F, ed)\n",
    "        return out\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, block, shortcut=None):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.shortcut = shortcut or (lambda x: x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x) + self.shortcut(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c4fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "class ThompsonSamplingFeatureMap(nn.Module):\n",
    "    def __init__(self, output_dim, user_dim, user_embed_dim, preference_embed_dim, \n",
    "                hidden_dim=64, max_users=10000, dropout=0.1, lambda0=1e-5):\n",
    "        super().__init__()\n",
    "        self.user_info_embedding = CategorialEmbedding(user_dim, max_users, embedding_dim=user_embed_dim)\n",
    "        self.preference_embedding = nn.Embedding(max_users, embedding_dim=preference_embed_dim)\n",
    "        \n",
    "        concat_dim = (user_dim*user_embed_dim) + preference_embed_dim\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(concat_dim, hidden_dim)\n",
    "            ,nn.ReLU()\n",
    "            ,ResidualConnection(nn.Sequential(\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "                ,nn.ReLU()\n",
    "                ,nn.Dropout(dropout)\n",
    "                )\n",
    "            )\n",
    "            # ,ResidualConnection(nn.Sequential(\n",
    "            #     nn.BatchNorm1d(hidden_dim),\n",
    "            #     nn.Linear(hidden_dim, hidden_dim)\n",
    "            #     ,nn.ReLU()\n",
    "            #     ,nn.Dropout(dropout)\n",
    "            #     )\n",
    "            # )\n",
    "        )\n",
    "        \n",
    "        # head\n",
    "        self.output_dim = output_dim\n",
    "        self.tdim = output_dim * (output_dim+1)//2\n",
    "        \n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "        self.mu_head = nn.Linear(hidden_dim, output_dim)\n",
    "        self.triu_head = nn.Linear(hidden_dim, self.tdim)\n",
    "        \n",
    "        # cache diagonal indices in the triu-vector\n",
    "        self.lambda0 = lambda0\n",
    "        di_rows, di_cols = torch.triu_indices(output_dim, output_dim, offset=0)\n",
    "        diag_mask = (di_rows == di_cols)\n",
    "        self.register_buffer(\"diag_mask\", diag_mask)  # shape [tdim] : (register_buffer : 역전파로 업데이트되지 않지만 모델과 함께 저장/로드(move to cuda 등)되는 값)\n",
    "\n",
    "    def vec_to_triu(self, vec, k):\n",
    "        \"\"\"\n",
    "        vec: (..., k*(k+1)//2)\n",
    "        return: (..., k, k) upper-triangular matrix (zeros elsewhere)\n",
    "        \"\"\"\n",
    "        idx = torch.triu_indices(k, k, device=vec.device)\n",
    "        out = vec.new_zeros(*vec.shape[:-1], k, k)\n",
    "        out[..., idx[0], idx[1]] = vec\n",
    "        return out\n",
    "\n",
    "    def forward(self, user_vec:torch.LongTensor, user_id:torch.LongTensor):\n",
    "        batch_size = user_vec.shape[:-1]\n",
    "        \n",
    "        user_emb = self.user_info_embedding(user_vec).view(*batch_size, -1)\n",
    "        pref_emb = self.preference_embedding(user_id).view(*batch_size, -1)\n",
    "        x_concat = torch.cat([user_emb, pref_emb], dim=-1)\n",
    "        \n",
    "        # backbone\n",
    "        x_latent = self.backbone(x_concat)\n",
    "        \n",
    "        # mu\n",
    "        mu = self.mu_head(x_latent)\n",
    "        \n",
    "        # U\n",
    "        raw_triu = self.triu_head(x_latent)  # unconstrained\n",
    "        diag_part = raw_triu[..., self.diag_mask]\n",
    "        off_part  = raw_triu[..., ~self.diag_mask]\n",
    "        \n",
    "        # constrain diagonal: positive with softplus + epsilon\n",
    "        diag_part = self.softplus(diag_part) + self.lambda0\n",
    "        \n",
    "        U_triu = raw_triu.clone()       # .clone()도 gradient 전파 가능 # 파이토치에서 inplace 연산은 그래프 추적 중에 이전 연산에 영향을 줄 수 있기 때문에 종종 에러를 발생시킨다.\n",
    "        U_triu[..., self.diag_mask] = diag_part\n",
    "        U_triu[..., ~self.diag_mask] = off_part\n",
    "        \n",
    "        U = self.vec_to_triu(U_triu, self.output_dim)   # (..., k, k)\n",
    "        Lambda = U.transpose(-2, -1) @ U \n",
    "        \n",
    "        return mu, U, Lambda\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    def U_to_L(self, U:torch.Tensor):\n",
    "        \"\"\"\n",
    "        Given U (upper of precision), compute Sigma = (U^T U)^{-1} without explicit inverse.\n",
    "        Solve (U^T) Y = I  ->  L = U^{-T}\n",
    "        \"\"\"\n",
    "        k = U.shape[-1]\n",
    "        I = torch.eye(k, device=U.device, dtype=U.dtype).expand(U.shape[:-2] + (k, k))\n",
    "        # U^T is lower-triangular\n",
    "        L = torch.linalg.solve_triangular(U.transpose(-2, -1), I, upper=False, left=True)\n",
    "        return L\n",
    "    \n",
    "    def gaussian(self, mu, U, requires_grad=False):\n",
    "        ctx = torch.enable_grad() if requires_grad else torch.no_grad()\n",
    "        with ctx:\n",
    "            L = self.U_to_L(U)\n",
    "            Sigma = L @ L.transpose(-2,-1)\n",
    "        return mu, Sigma\n",
    "    \n",
    "    def forward_gaussian(self, user_vec:torch.LongTensor, user_id:torch.LongTensor):\n",
    "        mu, U, Lambda = self.forward(user_vec, user_id)\n",
    "        return self.gaussian(mu, U)\n",
    "    \n",
    "    def sampling(self, mu, U, n_samples=1, requires_grad=False):\n",
    "        ctx = torch.enable_grad() if requires_grad else torch.no_grad()\n",
    "        \n",
    "        with ctx:\n",
    "            L = self.U_to_L(U)\n",
    "            multivariate_gaussian_dist = torch.distributions.MultivariateNormal(loc=mu, scale_tril=L)\n",
    "            # multivariate_gaussian_dist = torch.distributions.MultivariateNormal(loc=mu, precision_matrix=Lambda)\n",
    "            samples = multivariate_gaussian_dist.rsample((n_samples,))  # (n_sample, batch , dim) rsample: 미분가능, sample: 미분불가\n",
    "            samples = torch.movedim(samples, 0, -2) # (batch, n_sample, dim)\n",
    "        return samples\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    def forward_sampling(self, user_vec:torch.LongTensor, user_id:torch.LongTensor, n_samples=1):\n",
    "        \"\"\"\n",
    "        Thompson-style sampling: c = mu + U^{-T} z,  z ~ N(0,I)\n",
    "        returns (..., n_samples, k)\n",
    "        \"\"\"\n",
    "        mu, U, Lambda = self.forward(user_vec, user_id)\n",
    "        return self.sampling(mu, U)\n",
    "\n",
    "####################################################################################################\n",
    "# users[0].id\n",
    "# users[0].user_vec\n",
    "# users[0].user_vec_group\n",
    "# model = ThompsonSamplingFeatureMap(COMPATIBILITY_DIM, users[0].user_dim,\n",
    "#                                    user_embed_dim=3, preference_embed_dim=8)\n",
    "# a = torch.LongTensor([users[0].user_vec]).repeat(5,1)\n",
    "# b = torch.LongTensor([[users[0].id]]).repeat(5,1)\n",
    "# a = torch.LongTensor([users[0].user_vec])\n",
    "# b = torch.LongTensor([[users[0].id]])\n",
    "\n",
    "# model(a,b)[0].shape\n",
    "# model(a,b)[1].shape\n",
    "# model(a,b)[2].shape\n",
    "# model.forward_gaussian(a,b)\n",
    "# model.forward_sample(a,b, n_samples=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eae07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Global Optima Matching\n",
    "# 1) hungarian matching\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "def hungarian(similarity_matrix):\n",
    "    S_max = similarity_matrix.max()         # 행렬 최대값\n",
    "    cost_matrix = S_max - similarity_matrix     # 비용행렬\n",
    "    np.fill_diagonal(cost_matrix, np.inf)\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    optimal_matching_values = similarity_matrix[row_ind, col_ind]\n",
    "    matching_result = np.concatenate([row_ind[np.newaxis,...].T, col_ind[np.newaxis,...].T], axis=-1).T\n",
    "    return matching_result, optimal_matching_values\n",
    "\n",
    "\n",
    "# 2) 일반 그래프 최대 가중 매칭(Blossom)\n",
    "import networkx as nx\n",
    "def blossom_max_weight_matching(similarity_matrix):\n",
    "    n = similarity_matrix.shape[0]\n",
    "    G = nx.Graph()\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            w = similarity_matrix[i, j]\n",
    "            if np.isfinite(w):\n",
    "                G.add_edge(i, j, weight=float(w))\n",
    "    # 최대 가중 + 최대 카디널리티(가능한 많이 페어링)\n",
    "    matching = nx.algorithms.matching.max_weight_matching(\n",
    "        G, maxcardinality=True, weight='weight'\n",
    "    )\n",
    "\n",
    "    # matching result processing\n",
    "    pair_half = list(matching)  # (i, j) 튜플들의 집합\n",
    "    pair_half_np =  np.stack(pair_half)\n",
    "    pair_np = np.concatenate([pair_half_np, pair_half_np[:, ::-1]], axis=0)\n",
    "    sort_idx = np.argsort(pair_np[:,0])\n",
    "    matching_result = pair_np[sort_idx].T\n",
    "\n",
    "    # optimal_matching_values\n",
    "    row_ind = matching_result[0]\n",
    "    col_ind = matching_result[1]\n",
    "    optimal_matching_values = similarity_matrix[row_ind, col_ind]\n",
    "    return matching_result, optimal_matching_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27abac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# Local Optima Matching\n",
    "import numpy as np\n",
    "def greedy_matching(similarity_matrix):\n",
    "    \"\"\"\n",
    "    유사도 행렬을 한 번 정렬한 후 탐욕적으로 매칭을 수행합니다.\n",
    "    O(N^2 log N) 복잡도로, O(N^3)에 가까운 반복적 argmax 방식보다 빠릅니다.\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (np.ndarray): N x N 유사도 행렬.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (matching_result, optimal_matching_values)\n",
    "    \"\"\"\n",
    "    N = similarity_matrix.shape[0]\n",
    "\n",
    "    # 1. 유사도 행렬에서 매칭 후보 쌍(i, j)만 추출합니다. (i < j 조건)\n",
    "    # 삼각 행렬 인덱스 추출 (자기 자신과의 매칭 방지)\n",
    "    iu = np.triu_indices(N, k=1)\n",
    "\n",
    "    # 2. 모든 유사도 값을 추출하고 내림차순으로 정렬합니다.\n",
    "    flat_similarities = similarity_matrix[iu]\n",
    "\n",
    "    # 유사도 값이 높은 순서(내림차순)로 정렬하기 위한 인덱스\n",
    "    sort_idx = np.argsort(flat_similarities)[::-1]\n",
    "\n",
    "    # 정렬된 유사도 값과 해당 쌍의 인덱스\n",
    "    sorted_values = flat_similarities[sort_idx]\n",
    "    sorted_pairs = np.column_stack(iu)[sort_idx] # (row, col) 쌍\n",
    "\n",
    "    # 매칭 결과를 저장할 리스트와 매칭된 유저 집합\n",
    "    matched_pairs = []\n",
    "    matched_values = []\n",
    "    matched_indices = set()\n",
    "\n",
    "    # 3. 정렬된 쌍을 순회하며 매칭합니다. (O(N^2) 순회)\n",
    "    for value, (i, j) in zip(sorted_values, sorted_pairs):\n",
    "        # 이미 i 또는 j가 매칭되었는지 확인 (O(1) set lookup)\n",
    "        if i not in matched_indices and j not in matched_indices:\n",
    "            # 매칭 확정\n",
    "            matched_pairs.append((i, j))\n",
    "            matched_values.append(value)\n",
    "\n",
    "            # 유저 제거 (논리적 제거)\n",
    "            matched_indices.add(i)\n",
    "            matched_indices.add(j)\n",
    "\n",
    "            # 매칭이 절반 이상 완료되면 종료\n",
    "            if len(matched_indices) >= N - 1:\n",
    "                break\n",
    "\n",
    "    if not matched_pairs:\n",
    "        return np.array([[], []]), np.array([])\n",
    "\n",
    "    # 4. 결과 포맷팅 (이 부분은 이전 코드와 동일)\n",
    "    matched_pairs_np = np.array(matched_pairs)\n",
    "\n",
    "    # (i, j)뿐만 아니라 (j, i)도 포함\n",
    "    all_pairs = np.concatenate([matched_pairs_np, matched_pairs_np[:, ::-1]], axis=0)\n",
    "    all_values = np.concatenate([np.array(matched_values), np.array(matched_values)], axis=0)\n",
    "\n",
    "    # 행 인덱스(user_i) 기준으로 정렬\n",
    "    sort_idx = np.argsort(all_pairs[:, 0])\n",
    "    matching_result = all_pairs[sort_idx].T\n",
    "    optimal_matching_values = all_values[sort_idx]\n",
    "\n",
    "    return matching_result, optimal_matching_values\n",
    "\n",
    "\n",
    "\n",
    "class KNN_GreedyMatching():\n",
    "    def __init__(self, k=None):\n",
    "        self.k = k\n",
    "\n",
    "    def knn_greedy_matching(self, similarity_matrix):\n",
    "        \"\"\"\n",
    "        유사도 행렬을 기반으로, 각 유저의 k명의 가장 유사한 후보군 내에서만\n",
    "        탐욕적으로 매칭을 수행하는 근사 매칭 함수입니다.\n",
    "\n",
    "        Args:\n",
    "            similarity_matrix (np.ndarray): N x N 유사도 행렬.\n",
    "            k (int): 각 유저가 고려할 이웃의 최대 수.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (matching_result, optimal_matching_values)\n",
    "                matching_result (np.ndarray): 매칭된 쌍의 인덱스 (2, M).\n",
    "                optimal_matching_values (np.ndarray): 매칭된 쌍의 유사도 값 (M,).\n",
    "        \"\"\"\n",
    "        N = similarity_matrix.shape[0]\n",
    "        k = self.k\n",
    "        if k is None or (k >= N - 1):\n",
    "            # k가 N보다 크면 그냥 일반적인 탐욕적 매칭과 동일\n",
    "            return greedy_matching(similarity_matrix)\n",
    "\n",
    "        # 1. k-NN 기반 후보 쌍만 추출하여 정렬합니다.\n",
    "\n",
    "        # 1-1. 각 유저별 k명의 이웃 인덱스를 찾습니다.\n",
    "        # np.argsort는 기본적으로 오름차순. [::-1]로 내림차순 정렬 후, k개 선택.\n",
    "        # 자기 자신(가장 유사도 1.0)을 제외하고, 인접한 k개의 인덱스를 찾습니다.\n",
    "\n",
    "        # 유사도 행렬의 각 행(유저 i)에서 k+1번째로 유사한 인덱스까지\n",
    "        # 즉, 자기 자신(0순위) 포함 k+1개\n",
    "        k_plus_1_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1][:, :k+1]\n",
    "\n",
    "        # 1-2. 매칭 후보 (쌍) 리스트 생성\n",
    "        # 모든 유저 i에 대해, 그들의 k-NN 후보 j만을 추출합니다. (i < j 조건 만족)\n",
    "        # 이는 중복 쌍을 피하고 대칭성을 깨지 않기 위함입니다.\n",
    "\n",
    "        candidate_pairs = set()\n",
    "\n",
    "        for i in range(N):\n",
    "            # 유저 i의 k-NN 후보 목록\n",
    "            neighbors = k_plus_1_indices[i]\n",
    "\n",
    "            for j in neighbors:\n",
    "                # 1. 자기 자신과의 매칭 제외\n",
    "                if i == j:\n",
    "                    continue\n",
    "                # 2. 이미 (j, i)로 처리된 쌍을 피하기 위해 정규화된 쌍 (min, max)만 추가\n",
    "                pair = tuple(sorted((i, j)))\n",
    "                candidate_pairs.add(pair)\n",
    "\n",
    "        # 2. 후보 쌍의 유사도 값을 추출하고 내림차순으로 정렬합니다.\n",
    "\n",
    "        # 후보 쌍 (i, j)와 해당 유사도 값 추출\n",
    "        pairs_list = np.array(list(candidate_pairs))\n",
    "        flat_similarities = similarity_matrix[pairs_list[:, 0], pairs_list[:, 1]]\n",
    "\n",
    "        # 유사도 값이 높은 순서(내림차순)로 정렬하기 위한 인덱스\n",
    "        sort_idx = np.argsort(flat_similarities)[::-1]\n",
    "\n",
    "        sorted_values = flat_similarities[sort_idx]\n",
    "        sorted_pairs = pairs_list[sort_idx]\n",
    "\n",
    "        # 3. 탐욕적으로 매칭합니다. (나머지 과정은 optimized_greedy_matching과 동일)\n",
    "\n",
    "        matched_pairs = []\n",
    "        matched_values = []\n",
    "        matched_indices = set()\n",
    "\n",
    "        for value, (i, j) in zip(sorted_values, sorted_pairs):\n",
    "            if i not in matched_indices and j not in matched_indices:\n",
    "                # 매칭 확정\n",
    "                matched_pairs.append((i, j))\n",
    "                matched_values.append(value)\n",
    "\n",
    "                # 유저 제거\n",
    "                matched_indices.add(i)\n",
    "                matched_indices.add(j)\n",
    "\n",
    "                if len(matched_indices) >= N - 1:\n",
    "                    break\n",
    "\n",
    "        if not matched_pairs:\n",
    "            return np.array([[], []]), np.array([])\n",
    "\n",
    "        # 4. 결과 포맷팅\n",
    "        matched_pairs_np = np.array(matched_pairs)\n",
    "\n",
    "        # (i, j)뿐만 아니라 (j, i)도 포함하여 정렬\n",
    "        all_pairs = np.concatenate([matched_pairs_np, matched_pairs_np[:, ::-1]], axis=0)\n",
    "        all_values = np.concatenate([np.array(matched_values), np.array(matched_values)], axis=0)\n",
    "\n",
    "        # 행 인덱스(user_i) 기준으로 정렬\n",
    "        sort_idx = np.argsort(all_pairs[:, 0])\n",
    "        matching_result = all_pairs[sort_idx].T\n",
    "        optimal_matching_values = all_values[sort_idx]\n",
    "\n",
    "        return matching_result, optimal_matching_values\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d33a71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################################################\n",
    "# USER_DIM = 4\n",
    "# COMPATIBILITY_DIM = 16\n",
    "# USER_EMBEDDING_DIM = 3\n",
    "# PREFERENCE_EMBEDDING_DIM = 8\n",
    "# users = [AgentTS(user_dim=USER_DIM, compatibility_dim=COMPATIBILITY_DIM) for _ in range(6)]\n",
    "\n",
    "# # (True matching) -------------------------------------------------------------------\n",
    "# users_true_vec = np.array([user.true_compatibility for user in users])\n",
    "# users_true_vec\n",
    "\n",
    "# # # true net\n",
    "# # true_net = TrueCompatibilityNet(AgentTS.true_compatibility_dim)\n",
    "# # users_true_vec = true_net(users_true_vec)\n",
    "# SM_true = users_true_vec @ users_true_vec.T\n",
    "\n",
    "# # true_matching, true_matching_values = hungarian(SM_true)  # True matching\n",
    "# true_matching, true_matching_values = blossom_max_weight_matching(SM_true)  # True matching\n",
    "# print('< true_matching >')\n",
    "# print(true_matching)\n",
    "# print(true_matching_values)\n",
    "# # ------------------------------------------------------------------------------------\n",
    "\n",
    "# # sample matching\n",
    "# TS_model = ThompsonSamplingFeatureMap(COMPATIBILITY_DIM, USER_DIM, USER_EMBEDDING_DIM, PREFERENCE_EMBEDDING_DIM)\n",
    "\n",
    "# users_vec = torch.LongTensor( np.stack([user.user_vec for user in users]) )\n",
    "# users_id = torch.LongTensor( np.stack([[user.id] for user in users]) )\n",
    "\n",
    "# compatibility_samples = TS_model.forward_sample(users_vec, users_id).squeeze(-2)\n",
    "# compatibility_samples_np = compatibility_samples.detach().to('cpu').numpy()\n",
    "\n",
    "# SM_samples = compatibility_samples_np @ compatibility_samples_np.T\n",
    "\n",
    "# # sample_matching, sample_matching_values = hungarian(SM_samples)\n",
    "# sample_matching, sample_matching_values = blossom_max_weight_matching(SM_samples)\n",
    "# print('< sampling_matching >')\n",
    "# print(sample_matching)\n",
    "# print(sample_matching_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3699f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# After Maching, Revealed Rewards\n",
    "def revealed_reward(true_similarity_matrix, sample_matching, noise_std=0.1):\n",
    "    mu_rewards = true_similarity_matrix.copy()\n",
    "    # true_similarity_matrix_copy = true_similarity_matrix.copy()\n",
    "    # np.fill_diagonal(true_similarity_matrix_copy, -np.inf)\n",
    "    # np.exp(true_similarity_matrix_copy) / np.exp(true_similarity_matrix_copy).sum(axis=1)\n",
    "    # mu_rewards = 1/2*(np.exp(true_similarity_matrix_copy) / np.exp(true_similarity_matrix_copy).sum(axis=0)) + 1/2*(np.exp(true_similarity_matrix_copy) / np.exp(true_similarity_matrix_copy).sum(axis=0)).T\n",
    "\n",
    "    reward_noise_gen = rng.random(size=mu_rewards.shape)*noise_std-noise_std/2\n",
    "    reward_noise_gen = (reward_noise_gen + reward_noise_gen.T)/2\n",
    "    np.fill_diagonal(reward_noise_gen, 0)\n",
    "\n",
    "    revealed_rewards = mu_rewards+reward_noise_gen\n",
    "    # revealed_rewards = np.clip(mu_rewards+reward_noise_gen, a_min=0, a_max=1)\n",
    "    return revealed_rewards[sample_matching[0], sample_matching[1]]\n",
    "\n",
    "# rewards_obs = revealed_reward(SM_true, sample_matching)\n",
    "# print(f\"revealed rewards : {rewards_obs}\")\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "# rewards = revealed_reward(SM_true, sample_matching)\n",
    "# mu_target, Sigma_target = users[0].update(compatibility_samples_np[3], rewards[0], users[3].Sigma, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b90091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "\n",
    "def calculate_target_pair(idx_i:float, idx_j:int, \n",
    "                    reward:float, users:AgentTS, compatibility_samples:np.array, inplace:bool=True):\n",
    "    mu_target_i, Sigma_target_i = users[idx_i].update(compatibility_samples[idx_j], reward, users[idx_j].Sigma, inplace=inplace)\n",
    "    mu_target_j, Sigma_target_j = users[idx_j].update(compatibility_samples[idx_i], reward, users[idx_i].Sigma, inplace=inplace)\n",
    "    return (mu_target_i, Sigma_target_i), (mu_target_j, Sigma_target_j)\n",
    "    \n",
    "def calculate_targets(rewards, users, compatibility_samples, sample_matching, inplace=True):\n",
    "    mu_targets = [np.array([])]*len(users)\n",
    "    Sigma_targets = [np.array([])]*len(users)\n",
    "    for reward, match in zip(rewards, sample_matching.T):\n",
    "        idx_i = match[0]\n",
    "        idx_j = match[1]\n",
    "        \n",
    "        if len(mu_targets[idx_i]) ==0 and len(mu_targets[idx_j]) ==0:\n",
    "            (mu_target_i, Sigma_target_i), (mu_target_j, Sigma_target_j) = calculate_target_pair(idx_i, idx_j, reward, users, compatibility_samples, inplace)\n",
    "            mu_targets[idx_i] = mu_target_i\n",
    "            mu_targets[idx_j] = mu_target_j\n",
    "            Sigma_targets[idx_i] = Sigma_target_i\n",
    "            Sigma_targets[idx_j] = Sigma_target_j\n",
    "    return mu_targets, Sigma_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abbd1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "def kl_gaussian_full(mu0, Sigma0, mu1, Sigma1, eps=1e-6, reduction: str = \"mean\"):\n",
    "    \"\"\"\n",
    "    Multivariate Gaussian KL(N0 || N1)\n",
    "    N0 ~ N(mu0, Sigma0)\n",
    "    N1 ~ N(mu1, Sigma1)\n",
    "\n",
    "    mu0, mu1: (..., D)\n",
    "    Sigma0, Sigma1: (..., D, D)  (대칭, 양의정부호라고 가정)\n",
    "    \"\"\"\n",
    "    # 차원\n",
    "    D = mu0.shape[-1]\n",
    "\n",
    "    # 작은 값 더해서 수치적으로 안정화\n",
    "    eye = eps * torch.eye(D, device=mu0.device, dtype=mu0.dtype)\n",
    "    Sigma0 = Sigma0 + eye\n",
    "    Sigma1 = Sigma1 + eye\n",
    "\n",
    "    # Sigma1^{-1}\n",
    "    Sigma1_inv = torch.linalg.inv(Sigma1)\n",
    "\n",
    "    # trace(Sigma1^{-1} Sigma0)\n",
    "    # (..., D, D) x (..., D, D) -> (..., D, D) -> trace\n",
    "    term_trace = torch.einsum('...ij,...jk->...ik', Sigma1_inv, Sigma0)\n",
    "    term_trace = torch.einsum('...ii->...', term_trace)  # trace\n",
    "\n",
    "    # (mu1 - mu0)^T Sigma1^{-1} (mu1 - mu0)\n",
    "    diff = (mu1 - mu0).unsqueeze(-1)          # (..., D, 1)\n",
    "    mahal = torch.matmul(Sigma1_inv, diff)    # (..., D, 1)\n",
    "    mahal = torch.matmul(diff.transpose(-1, -2), mahal)  # (..., 1, 1)\n",
    "    term_quad = mahal.squeeze(-1).squeeze(-1)            # (...,)\n",
    "\n",
    "    # log det 부분: log(det(Sigma1)) - log(det(Sigma0))\n",
    "    # slogdet: sign, logabsdet\n",
    "    sign0, logdet0 = torch.linalg.slogdet(Sigma0)\n",
    "    sign1, logdet1 = torch.linalg.slogdet(Sigma1)\n",
    "    # sign이 -1이면 문제가 있는 covariance\n",
    "    # 여기선 양의정부호를 가정하므로 sign == 1이라고 가정\n",
    "    term_logdet = logdet1 - logdet0\n",
    "\n",
    "    # KL = 0.5 * (trace + quad - D + logdet1 - logdet0)\n",
    "    kl = 0.5 * (term_trace + term_quad - D + term_logdet)\n",
    "    \n",
    "    if reduction == \"mean\":\n",
    "        return kl.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return kl.sum()\n",
    "    else:  # 'none'\n",
    "        return kl\n",
    "\n",
    "\n",
    "# \n",
    "def kl_gaussian_prec_chol(\n",
    "    mu_target: torch.Tensor,       # (B, k)\n",
    "    cov_target: torch.Tensor,      # (B, k, k)   Σ_target\n",
    "    mu_pred: torch.Tensor,         # (B, k)      μ_hat\n",
    "    prec_chol_pred: torch.Tensor,  # (B, k, k)   U_hat : Σ_hat^{-1} = U^T U\n",
    "    reduction: str = \"mean\"\n",
    "):\n",
    "    \"\"\"\n",
    "    KL( N(mu_target, cov_target) || N(mu_pred, cov_pred) )을 계산.\n",
    "    cov_pred는 직접 입력하지 않고, prec_chol_pred (precision의 Cholesky factor)로부터 유도.\n",
    "\n",
    "    mu_target, mu_pred   : (B, k)\n",
    "    cov_target           : (B, k, k)\n",
    "    prec_chol_pred (U)   : (B, k, k),  Σ_hat^{-1} = U^T U\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "\n",
    "    B, k = mu_target.shape\n",
    "\n",
    "    # 1) Σ_hat^{-1} = Λ_hat = U^T U\n",
    "    precision_pred = prec_chol_pred.transpose(-1, -2) @ prec_chol_pred  # (B, k, k)\n",
    "\n",
    "    # 2) tr( Σ_hat^{-1} Σ_target )\n",
    "    #   term1 = tr( precision_pred @ cov_target )\n",
    "    #   trace(AB) = sum_ij A_ij B_ji\n",
    "    term1 = torch.einsum(\"bij,bji->b\", precision_pred, cov_target)\n",
    "\n",
    "    # 3) (μ_hat - μ_target)^T Σ_hat^{-1} (μ_hat - μ_target)\n",
    "    diff = (mu_pred - mu_target).unsqueeze(-1)       # (B, k, 1)\n",
    "    quad = diff.transpose(-1, -2) @ (precision_pred @ diff)  # (B, 1, 1)\n",
    "    term2 = quad.squeeze(-1).squeeze(-1)             # (B,)\n",
    "\n",
    "    # 4) log det Σ_hat  (prec_chol_pred는 Σ_hat^{-1}의 Cholesky factor)\n",
    "    #    log det Σ_hat^{-1} = 2 * sum(log(diag(U)))\n",
    "    diag_U = torch.diagonal(prec_chol_pred, dim1=-2, dim2=-1)  # (B, k)\n",
    "    logdet_prec_pred = 2.0 * torch.log(diag_U + eps).sum(-1)   # (B,)\n",
    "    logdet_cov_pred = -logdet_prec_pred                         # log det Σ_hat\n",
    "\n",
    "    # 5) log det Σ_target  (표준 Cholesky 사용)\n",
    "    #    cov_target = L L^T  → log det Σ_target = 2 * sum(log(diag(L)))\n",
    "    L_target = torch.linalg.cholesky(cov_target + eps * torch.eye(k, device=cov_target.device))\n",
    "    diag_L = torch.diagonal(L_target, dim1=-2, dim2=-1)         # (B, k)\n",
    "    logdet_cov_target = 2.0 * torch.log(diag_L + eps).sum(-1)   # (B,)\n",
    "\n",
    "    # 6) 전체 KL:\n",
    "    #   KL = 1/2 [ tr(Σ_hat^{-1} Σ_target)\n",
    "    #              + (μ_hat - μ_t)^T Σ_hat^{-1} (μ_hat - μ_t)\n",
    "    #              - k\n",
    "    #              + log( det Σ_hat / det Σ_target ) ]\n",
    "    term3 = logdet_cov_pred - logdet_cov_target\n",
    "\n",
    "    kl_per_sample = 0.5 * (term1 + term2 - k + term3)  # (B,)\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        return kl_per_sample.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return kl_per_sample.sum()\n",
    "    else:  # 'none'\n",
    "        return kl_per_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f3a8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1714c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_USERS = 100\n",
    "USER_DIM = 4\n",
    "USER_EMBEDDING_DIM = 4\n",
    "PREFERENCE_EMBEDDING_DIM = 32\n",
    "COMPATIBILITY_DIM = 12      # 실제 추정에 사용되는 궁합 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33f13e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< true_matching >\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      "  48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      "  72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      "  96 97 98 99]\n",
      " [29  3 20  1 61 41 23 93 28 40 44 35 71 64 34 88 58 49 84 57  2 42 25  6\n",
      "  96 22 73 43  8  0 82 55 89 62 14 11 60 91 95 76  9  5 21 27 10 69 48 80\n",
      "  46 17 66 67 97 85 81 31 59 19 16 56 36  4 33 98 13 92 50 51 86 45 99 12\n",
      "  74 26 72 87 39 94 90 83 47 54 30 79 18 53 68 75 15 32 78 37 65  7 77 38\n",
      "  24 52 63 70]]\n",
      "[0.41405944 0.32123309 0.42952519 0.32123309 0.51030631 0.3575322\n",
      " 0.49832681 0.31561988 0.484194   0.46539115 0.38188947 0.35374972\n",
      " 0.35302433 0.3947115  0.45551178 0.38280822 0.35408966 0.35386071\n",
      " 0.50274316 0.21777204 0.42952519 0.4164008  0.31220032 0.49832681\n",
      " 0.36390501 0.31220032 0.49332942 0.40498398 0.484194   0.41405944\n",
      " 0.27649431 0.36033484 0.39571135 0.66036606 0.45551178 0.35374972\n",
      " 0.40035184 0.38133865 0.3589923  0.62607617 0.46539115 0.3575322\n",
      " 0.4164008  0.40498398 0.38188947 0.35868116 0.46503251 0.38994203\n",
      " 0.46503251 0.35386071 0.2874191  0.42277189 0.40244145 0.41574869\n",
      " 0.31667251 0.36033484 0.47268156 0.21777204 0.35408966 0.47268156\n",
      " 0.40035184 0.51030631 0.66036606 0.49014294 0.3947115  0.41474963\n",
      " 0.2874191  0.42277189 0.42994425 0.35868116 0.37339    0.35302433\n",
      " 0.50726853 0.49332942 0.50726853 0.33893394 0.62607617 0.39221328\n",
      " 0.48161236 0.46177715 0.38994203 0.31667251 0.27649431 0.46177715\n",
      " 0.50274316 0.41574869 0.42994425 0.33893394 0.38280822 0.39571135\n",
      " 0.48161236 0.38133865 0.41474963 0.31561988 0.39221328 0.3589923\n",
      " 0.36390501 0.40244145 0.49014294 0.37339   ]\n",
      "total return rewards (true) : 40.837\n"
     ]
    }
   ],
   "source": [
    "\n",
    "users = [AgentTS(user_dim=USER_DIM, compatibility_dim=COMPATIBILITY_DIM, forgetting_decay=1) for _ in range(N_USERS)]\n",
    "\n",
    "# (True matching) -------------------------------------------------------------------\n",
    "users_true_vec = np.array([user.true_compatibility for user in users])\n",
    "\n",
    "\n",
    "# # true net\n",
    "# true_net = TrueCompatibilityNet(AgentTS.true_compatibility_dim)\n",
    "# users_true_vec = true_net(users_true_vec)\n",
    "SM_true = users_true_vec @ users_true_vec.T\n",
    "\n",
    "# true_matching, true_matching_values = hungarian(SM_true)  # True matching\n",
    "np.round(SM_true,2)\n",
    "true_matching, true_matching_values = blossom_max_weight_matching(SM_true)  # True matching\n",
    "print('< true_matching >')\n",
    "print(true_matching)\n",
    "print(true_matching_values)\n",
    "print(f\"total return rewards (true) : {true_matching_values.sum():.3f}\")\n",
    "# ------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea8c0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "## (LinTS) ########################################################################################\n",
    "# reset params\n",
    "[user.reset_params() for user in users]\n",
    "\n",
    "#  matching\n",
    "rewards_obs_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06c216fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed4c58cc2a44b57a040f8abc8610047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matching\n",
    "matching_function = greedy_matching\n",
    "for _ in tqdm(range(1800)):\n",
    "    # # user vector / id\n",
    "    # users_vec = np.stack([user.user_vec for user in users])\n",
    "    # users_id = np.stack([[user.id] for user in users])\n",
    "    \n",
    "    # sampling \n",
    "    users_samples_vec = np.stack([user.sampling() for user in users])\n",
    "    \n",
    "    # calculate similarity matrix\n",
    "    SM_samples = users_samples_vec @ users_samples_vec.T\n",
    "    # np.round(SM_samples,2)\n",
    "    \n",
    "    # users matching\n",
    "    sample_matching, sample_matching_values = matching_function(SM_samples)\n",
    "    \n",
    "    # obseve rewards\n",
    "    rewards_obs = revealed_reward(SM_true, sample_matching, noise_std=0.2)\n",
    "    rewards_obs\n",
    "    rewards_obs_list.append(np.sum(rewards_obs))\n",
    "    \n",
    "    # Users parameter Update\n",
    "    calculate_targets(rewards_obs, users, users_samples_vec, sample_matching, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92b173e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(rewards_obs_list)\n",
    "plt.axhline(np.sum(true_matching_values), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84bf0f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< sampling_matching >\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      "  48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      "  72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      "  96 97 98 99]\n",
      " [29  3 20  1 61 96 23 32 28 40 44 19 71 64 34 38 58 59 84 11  2 60 89  6\n",
      "  57 94 30 43  8  0 26 67  7 62 14 91 75 66 15 76  9 42 41 27 10 69 48 80\n",
      "  46 81 54 88 97 73 50 93 95 24 16 17 21  4 33 82 13 92 37 31 86 45 99 12\n",
      "  74 53 72 36 39 98 90 83 47 49 63 79 18 87 68 85 51 22 78 35 65 55 25 56\n",
      "   5 52 77 70]]\n",
      "[ 0.41306116  0.31380791  0.43033673  0.31380791  0.51027186  0.30727632\n",
      "  0.49390174 -0.00374795  0.48454555  0.46232327  0.37021346  0.21777504\n",
      "  0.34431137  0.38837523  0.44955903  0.37711661  0.34758629  0.3569596\n",
      "  0.50283973  0.21777504  0.43033673  0.33865447  0.28202922  0.49390174\n",
      "  0.08308173  0.32417783  0.25276094  0.40236718  0.48454555  0.41306116\n",
      "  0.25276094  0.23683089 -0.00374795  0.65780057  0.44955903  0.37948454\n",
      "  0.16240479  0.37284831  0.37711661  0.62620131  0.46232327  0.36491397\n",
      "  0.36491397  0.40236718  0.37021346  0.34212492  0.46590511  0.38909881\n",
      "  0.46590511  0.09964643  0.12500741  0.05814795  0.39742981  0.21211501\n",
      "  0.12500741  0.40575206  0.34871253  0.08308173  0.34758629  0.3569596\n",
      "  0.33865447  0.51027186  0.65780057  0.34870469  0.38837523  0.41492575\n",
      "  0.37284831  0.23683089  0.4288177   0.34212492  0.36375084  0.34431137\n",
      "  0.50649498  0.21211501  0.50649498  0.16240479  0.62620131  0.08544279\n",
      "  0.47913652  0.46422643  0.38909881  0.09964643  0.34870469  0.46422643\n",
      "  0.50283973  0.30971551  0.4288177   0.30971551  0.05814795  0.28202922\n",
      "  0.47913652  0.37948454  0.41492575  0.40575206  0.32417783  0.34871253\n",
      "  0.30727632  0.39742981  0.08544279  0.36375084]\n",
      "total return rewards (training) : 34.990\n"
     ]
    }
   ],
   "source": [
    "# sample matching\n",
    "users_samples_vec = np.stack([user.mu for user in users])\n",
    "SM_samples = users_samples_vec @ users_samples_vec.T\n",
    "\n",
    "# sample_matching, sample_matching_values = hungarian(SM_samples)\n",
    "sample_matching, sample_matching_values = blossom_max_weight_matching(SM_samples)\n",
    "print('< sampling_matching >')\n",
    "print(sample_matching)\n",
    "print(sample_matching_values)\n",
    "print(f\"total return rewards (training) : {sample_matching_values.sum():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7721ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< true_matching >\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      "  48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      "  72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      "  96 97 98 99]\n",
      " [29  3 20  1 61 41 23 93 28 40 44 35 71 64 34 88 58 49 84 57  2 42 25  6\n",
      "  96 22 73 43  8  0 82 55 89 62 14 11 60 91 95 76  9  5 21 27 10 69 48 80\n",
      "  46 17 66 67 97 85 81 31 59 19 16 56 36  4 33 98 13 92 50 51 86 45 99 12\n",
      "  74 26 72 87 39 94 90 83 47 54 30 79 18 53 68 75 15 32 78 37 65  7 77 38\n",
      "  24 52 63 70]]\n",
      "[0.41405944 0.32123309 0.42952519 0.32123309 0.51030631 0.3575322\n",
      " 0.49832681 0.31561988 0.484194   0.46539115 0.38188947 0.35374972\n",
      " 0.35302433 0.3947115  0.45551178 0.38280822 0.35408966 0.35386071\n",
      " 0.50274316 0.21777204 0.42952519 0.4164008  0.31220032 0.49832681\n",
      " 0.36390501 0.31220032 0.49332942 0.40498398 0.484194   0.41405944\n",
      " 0.27649431 0.36033484 0.39571135 0.66036606 0.45551178 0.35374972\n",
      " 0.40035184 0.38133865 0.3589923  0.62607617 0.46539115 0.3575322\n",
      " 0.4164008  0.40498398 0.38188947 0.35868116 0.46503251 0.38994203\n",
      " 0.46503251 0.35386071 0.2874191  0.42277189 0.40244145 0.41574869\n",
      " 0.31667251 0.36033484 0.47268156 0.21777204 0.35408966 0.47268156\n",
      " 0.40035184 0.51030631 0.66036606 0.49014294 0.3947115  0.41474963\n",
      " 0.2874191  0.42277189 0.42994425 0.35868116 0.37339    0.35302433\n",
      " 0.50726853 0.49332942 0.50726853 0.33893394 0.62607617 0.39221328\n",
      " 0.48161236 0.46177715 0.38994203 0.31667251 0.27649431 0.46177715\n",
      " 0.50274316 0.41574869 0.42994425 0.33893394 0.38280822 0.39571135\n",
      " 0.48161236 0.38133865 0.41474963 0.31561988 0.39221328 0.3589923\n",
      " 0.36390501 0.40244145 0.49014294 0.37339   ]\n",
      "total return rewards (true) : 40.837\n"
     ]
    }
   ],
   "source": [
    "print('< true_matching >')\n",
    "print(true_matching)\n",
    "print(true_matching_values)\n",
    "print(f\"total return rewards (true) : {true_matching_values.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "704d042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# (DeepTS) #########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66ac8320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< true_matching >\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      "  48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      "  72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      "  96 97 98 99]\n",
      " [66 12 75 27 97 78 32 91 46 13 20 15  1  9 25 11 22 82 56 61 10 23 16 21\n",
      "  87 14 81  3 50 80 99 74  6 69 84 77 53 58 95 49 83 73 62 67 94 51  8 63\n",
      "  86 39 28 45 71 36 92 57 18 55 37 85 96 19 42 47 70 68  0 43 65 33 64 52\n",
      "  90 41 31  2 79 35  5 76 29 26 17 40 34 59 48 24 93 98 72  7 54 88 44 38\n",
      "  60  4 89 30]]\n",
      "[0.31706449 0.12613176 0.2586174  0.29204334 0.58486178 0.51581587\n",
      " 0.23446946 0.60495221 0.36157614 0.38401057 0.26247353 0.44821427\n",
      " 0.12613176 0.38401057 0.38288849 0.44821427 0.35907654 0.39309183\n",
      " 0.2857028  0.31786953 0.26247353 0.34687454 0.35907654 0.34687454\n",
      " 0.45511708 0.38288849 0.32313501 0.29204334 0.38084125 0.3858602\n",
      " 0.5090078  0.32746659 0.23446946 0.58180718 0.45004826 0.47036315\n",
      " 0.26481879 0.36498532 0.5273215  0.35824886 0.2668614  0.45506998\n",
      " 0.42600992 0.39046112 0.56824532 0.30695747 0.36157614 0.47685757\n",
      " 0.34139303 0.35824886 0.38084125 0.30695747 0.43918289 0.26481879\n",
      " 0.29852723 0.45367018 0.2857028  0.45367018 0.36498532 0.3399464\n",
      " 0.37099927 0.31786953 0.42600992 0.47685757 0.2974511  0.50009157\n",
      " 0.31706449 0.39046112 0.50009157 0.58180718 0.2974511  0.43918289\n",
      " 0.36115652 0.45506998 0.32746659 0.2586174  0.40675226 0.47036315\n",
      " 0.51581587 0.40675226 0.3858602  0.32313501 0.39309183 0.2668614\n",
      " 0.45004826 0.3399464  0.34139303 0.45511708 0.54531969 0.47427804\n",
      " 0.36115652 0.60495221 0.29852723 0.54531969 0.56824532 0.5273215\n",
      " 0.37099927 0.58486178 0.47427804 0.5090078 ]\n",
      "total return rewards (true) : 39.188\n"
     ]
    }
   ],
   "source": [
    "users = [AgentTS(user_dim=USER_DIM, compatibility_dim=COMPATIBILITY_DIM, forgetting_decay=1) for _ in range(N_USERS)]\n",
    "\n",
    "# (True matching) -------------------------------------------------------------------\n",
    "users_true_vec = np.array([user.true_compatibility for user in users])\n",
    "\n",
    "\n",
    "# # true net\n",
    "# true_net = TrueCompatibilityNet(AgentTS.true_compatibility_dim)\n",
    "# users_true_vec = true_net(users_true_vec)\n",
    "SM_true = users_true_vec @ users_true_vec.T\n",
    "\n",
    "# true_matching, true_matching_values = hungarian(SM_true)  # True matching\n",
    "np.round(SM_true,2)\n",
    "true_matching, true_matching_values = blossom_max_weight_matching(SM_true)  # True matching\n",
    "print('< true_matching >')\n",
    "print(true_matching)\n",
    "print(true_matching_values)\n",
    "print(f\"total return rewards (true) : {true_matching_values.sum():.3f}\")\n",
    "# ------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bff1e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reset params\n",
    "[user.reset_params() for user in users]\n",
    "\n",
    "# sample matching\n",
    "TS_model = ThompsonSamplingFeatureMap(COMPATIBILITY_DIM, USER_DIM, USER_EMBEDDING_DIM, PREFERENCE_EMBEDDING_DIM)\n",
    "optimizer = optim.Adam(TS_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da2af908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize ----------------------------------------------------------------------------------\n",
    "# for i in range(1000):\n",
    "#     users_vec = torch.LongTensor( np.stack([user.user_vec for user in users]) )\n",
    "#     users_id = torch.LongTensor( np.stack([[user.id] for user in users]) )\n",
    "#     # forward : mu, U, Lambda\n",
    "#     mu_Hat, U_Hat, Lambda_Hat = TS_model(users_vec, users_id) # Gradient\n",
    "\n",
    "#     mu_targets_tensor = torch.FloatTensor([user.mu for user in users])\n",
    "#     Sigma_targets_tensor = torch.FloatTensor([user.Sigma for user in users])\n",
    "\n",
    "#     # TS-Model Update (KL-divergence Loss)\n",
    "#     loss = kl_gaussian_prec_chol(mu_targets_tensor, Sigma_targets_tensor, mu_Hat, U_Hat, reduction='mean')\n",
    "#     loss = kl_gaussian_full(mu_targets_tensor, Sigma_targets_tensor, mu_Hat, torch.linalg.inv(Lambda_Hat), reduction='mean')\n",
    "    \n",
    "#     # model update\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(f\"(ITER: {i}) loss : {loss.detach().to('cpu').numpy():.3f}\")\n",
    "# ------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84e7813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching\n",
    "rewards_obs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e8473ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ITER: 0) loss : 15.008\n",
      "(ITER: 1) loss : 15.404\n",
      "(ITER: 2) loss : 16.020\n",
      "(ITER: 3) loss : 16.324\n",
      "(ITER: 4) loss : 16.098\n",
      "(ITER: 5) loss : 15.650\n",
      "(ITER: 6) loss : 14.883\n",
      "(ITER: 7) loss : 13.615\n",
      "(ITER: 8) loss : 12.648\n",
      "(ITER: 9) loss : 11.589\n",
      "(ITER: 10) loss : 10.489\n",
      "(ITER: 11) loss : 9.468\n",
      "(ITER: 12) loss : 8.614\n",
      "(ITER: 13) loss : 8.179\n",
      "(ITER: 14) loss : 7.603\n",
      "(ITER: 15) loss : 7.245\n",
      "(ITER: 16) loss : 6.896\n",
      "(ITER: 17) loss : 6.564\n",
      "(ITER: 18) loss : 6.384\n",
      "(ITER: 19) loss : 6.053\n",
      "(ITER: 20) loss : 5.916\n",
      "(ITER: 21) loss : 5.843\n",
      "(ITER: 22) loss : 5.710\n",
      "(ITER: 23) loss : 5.243\n",
      "(ITER: 24) loss : 5.339\n",
      "(ITER: 25) loss : 5.100\n",
      "(ITER: 26) loss : 5.140\n",
      "(ITER: 27) loss : 4.881\n",
      "(ITER: 28) loss : 4.795\n",
      "(ITER: 29) loss : 4.704\n",
      "(ITER: 30) loss : 4.567\n",
      "(ITER: 31) loss : 4.453\n",
      "(ITER: 32) loss : 4.340\n",
      "(ITER: 33) loss : 4.333\n",
      "(ITER: 34) loss : 4.274\n",
      "(ITER: 35) loss : 4.313\n",
      "(ITER: 36) loss : 4.266\n",
      "(ITER: 37) loss : 4.185\n",
      "(ITER: 38) loss : 4.038\n",
      "(ITER: 39) loss : 3.960\n",
      "(ITER: 40) loss : 3.907\n",
      "(ITER: 41) loss : 3.857\n",
      "(ITER: 42) loss : 3.836\n",
      "(ITER: 43) loss : 3.747\n",
      "(ITER: 44) loss : 3.718\n",
      "(ITER: 45) loss : 3.698\n",
      "(ITER: 46) loss : 3.676\n",
      "(ITER: 47) loss : 3.581\n",
      "(ITER: 48) loss : 3.559\n",
      "(ITER: 49) loss : 3.605\n",
      "(ITER: 50) loss : 3.484\n",
      "(ITER: 51) loss : 3.470\n",
      "(ITER: 52) loss : 3.422\n",
      "(ITER: 53) loss : 3.418\n",
      "(ITER: 54) loss : 3.350\n",
      "(ITER: 55) loss : 3.388\n",
      "(ITER: 56) loss : 3.303\n",
      "(ITER: 57) loss : 3.275\n",
      "(ITER: 58) loss : 3.282\n",
      "(ITER: 59) loss : 3.256\n",
      "(ITER: 60) loss : 3.236\n",
      "(ITER: 61) loss : 3.148\n",
      "(ITER: 62) loss : 3.183\n",
      "(ITER: 63) loss : 3.152\n",
      "(ITER: 64) loss : 3.124\n",
      "(ITER: 65) loss : 3.132\n",
      "(ITER: 66) loss : 3.113\n",
      "(ITER: 67) loss : 3.053\n",
      "(ITER: 68) loss : 3.059\n",
      "(ITER: 69) loss : 3.015\n",
      "(ITER: 70) loss : 3.014\n",
      "(ITER: 71) loss : 3.007\n",
      "(ITER: 72) loss : 3.002\n",
      "(ITER: 73) loss : 2.932\n",
      "(ITER: 74) loss : 2.935\n",
      "(ITER: 75) loss : 2.895\n",
      "(ITER: 76) loss : 2.896\n",
      "(ITER: 77) loss : 2.873\n",
      "(ITER: 78) loss : 2.877\n",
      "(ITER: 79) loss : 2.853\n",
      "(ITER: 80) loss : 2.847\n",
      "(ITER: 81) loss : 2.828\n",
      "(ITER: 82) loss : 2.806\n",
      "(ITER: 83) loss : 2.809\n",
      "(ITER: 84) loss : 2.762\n",
      "(ITER: 85) loss : 2.755\n",
      "(ITER: 86) loss : 2.732\n",
      "(ITER: 87) loss : 2.729\n",
      "(ITER: 88) loss : 2.739\n",
      "(ITER: 89) loss : 2.721\n",
      "(ITER: 90) loss : 2.697\n",
      "(ITER: 91) loss : 2.686\n",
      "(ITER: 92) loss : 2.685\n",
      "(ITER: 93) loss : 2.649\n",
      "(ITER: 94) loss : 2.685\n",
      "(ITER: 95) loss : 2.647\n",
      "(ITER: 96) loss : 2.625\n",
      "(ITER: 97) loss : 2.605\n",
      "(ITER: 98) loss : 2.621\n",
      "(ITER: 99) loss : 2.574\n",
      "(ITER: 100) loss : 2.550\n",
      "(ITER: 101) loss : 2.574\n",
      "(ITER: 102) loss : 2.558\n",
      "(ITER: 103) loss : 2.522\n",
      "(ITER: 104) loss : 2.537\n",
      "(ITER: 105) loss : 2.500\n",
      "(ITER: 106) loss : 2.524\n",
      "(ITER: 107) loss : 2.524\n",
      "(ITER: 108) loss : 2.479\n",
      "(ITER: 109) loss : 2.486\n",
      "(ITER: 110) loss : 2.463\n",
      "(ITER: 111) loss : 2.482\n",
      "(ITER: 112) loss : 2.457\n",
      "(ITER: 113) loss : 2.448\n",
      "(ITER: 114) loss : 2.448\n",
      "(ITER: 115) loss : 2.453\n",
      "(ITER: 116) loss : 2.417\n",
      "(ITER: 117) loss : 2.426\n",
      "(ITER: 118) loss : 2.403\n",
      "(ITER: 119) loss : 2.404\n",
      "(ITER: 120) loss : 2.398\n",
      "(ITER: 121) loss : 2.362\n",
      "(ITER: 122) loss : 2.377\n",
      "(ITER: 123) loss : 2.359\n",
      "(ITER: 124) loss : 2.341\n",
      "(ITER: 125) loss : 2.343\n",
      "(ITER: 126) loss : 2.346\n",
      "(ITER: 127) loss : 2.334\n",
      "(ITER: 128) loss : 2.316\n",
      "(ITER: 129) loss : 2.314\n",
      "(ITER: 130) loss : 2.286\n",
      "(ITER: 131) loss : 2.322\n",
      "(ITER: 132) loss : 2.287\n",
      "(ITER: 133) loss : 2.270\n",
      "(ITER: 134) loss : 2.266\n",
      "(ITER: 135) loss : 2.265\n",
      "(ITER: 136) loss : 2.262\n",
      "(ITER: 137) loss : 2.280\n",
      "(ITER: 138) loss : 2.246\n",
      "(ITER: 139) loss : 2.253\n",
      "(ITER: 140) loss : 2.219\n",
      "(ITER: 141) loss : 2.210\n",
      "(ITER: 142) loss : 2.221\n",
      "(ITER: 143) loss : 2.206\n",
      "(ITER: 144) loss : 2.210\n",
      "(ITER: 145) loss : 2.220\n",
      "(ITER: 146) loss : 2.181\n",
      "(ITER: 147) loss : 2.164\n",
      "(ITER: 148) loss : 2.167\n",
      "(ITER: 149) loss : 2.171\n",
      "(ITER: 150) loss : 2.142\n",
      "(ITER: 151) loss : 2.145\n",
      "(ITER: 152) loss : 2.127\n",
      "(ITER: 153) loss : 2.136\n",
      "(ITER: 154) loss : 2.117\n",
      "(ITER: 155) loss : 2.120\n",
      "(ITER: 156) loss : 2.125\n",
      "(ITER: 157) loss : 2.108\n",
      "(ITER: 158) loss : 2.104\n",
      "(ITER: 159) loss : 2.091\n",
      "(ITER: 160) loss : 2.121\n",
      "(ITER: 161) loss : 2.093\n",
      "(ITER: 162) loss : 2.113\n",
      "(ITER: 163) loss : 2.069\n",
      "(ITER: 164) loss : 2.088\n",
      "(ITER: 165) loss : 2.086\n",
      "(ITER: 166) loss : 2.053\n",
      "(ITER: 167) loss : 2.040\n",
      "(ITER: 168) loss : 2.048\n",
      "(ITER: 169) loss : 2.029\n",
      "(ITER: 170) loss : 2.036\n",
      "(ITER: 171) loss : 2.022\n",
      "(ITER: 172) loss : 2.005\n",
      "(ITER: 173) loss : 2.035\n",
      "(ITER: 174) loss : 1.992\n",
      "(ITER: 175) loss : 2.028\n",
      "(ITER: 176) loss : 1.985\n",
      "(ITER: 177) loss : 1.999\n",
      "(ITER: 178) loss : 1.982\n",
      "(ITER: 179) loss : 2.003\n",
      "(ITER: 180) loss : 1.976\n",
      "(ITER: 181) loss : 1.969\n",
      "(ITER: 182) loss : 1.969\n",
      "(ITER: 183) loss : 1.961\n",
      "(ITER: 184) loss : 1.950\n",
      "(ITER: 185) loss : 1.966\n",
      "(ITER: 186) loss : 1.950\n",
      "(ITER: 187) loss : 1.929\n",
      "(ITER: 188) loss : 1.923\n",
      "(ITER: 189) loss : 1.912\n",
      "(ITER: 190) loss : 1.931\n",
      "(ITER: 191) loss : 1.919\n",
      "(ITER: 192) loss : 1.894\n",
      "(ITER: 193) loss : 1.907\n",
      "(ITER: 194) loss : 1.878\n",
      "(ITER: 195) loss : 1.881\n",
      "(ITER: 196) loss : 1.885\n",
      "(ITER: 197) loss : 1.878\n",
      "(ITER: 198) loss : 1.875\n",
      "(ITER: 199) loss : 1.876\n",
      "(ITER: 200) loss : 1.862\n",
      "(ITER: 201) loss : 1.869\n",
      "(ITER: 202) loss : 1.872\n",
      "(ITER: 203) loss : 1.847\n",
      "(ITER: 204) loss : 1.856\n",
      "(ITER: 205) loss : 1.837\n",
      "(ITER: 206) loss : 1.841\n",
      "(ITER: 207) loss : 1.841\n",
      "(ITER: 208) loss : 1.813\n",
      "(ITER: 209) loss : 1.819\n",
      "(ITER: 210) loss : 1.830\n",
      "(ITER: 211) loss : 1.828\n",
      "(ITER: 212) loss : 1.813\n",
      "(ITER: 213) loss : 1.801\n",
      "(ITER: 214) loss : 1.818\n",
      "(ITER: 215) loss : 1.799\n",
      "(ITER: 216) loss : 1.797\n",
      "(ITER: 217) loss : 1.797\n",
      "(ITER: 218) loss : 1.777\n",
      "(ITER: 219) loss : 1.789\n",
      "(ITER: 220) loss : 1.762\n",
      "(ITER: 221) loss : 1.785\n",
      "(ITER: 222) loss : 1.765\n",
      "(ITER: 223) loss : 1.757\n",
      "(ITER: 224) loss : 1.747\n",
      "(ITER: 225) loss : 1.745\n",
      "(ITER: 226) loss : 1.780\n",
      "(ITER: 227) loss : 1.745\n",
      "(ITER: 228) loss : 1.741\n",
      "(ITER: 229) loss : 1.727\n",
      "(ITER: 230) loss : 1.710\n",
      "(ITER: 231) loss : 1.725\n",
      "(ITER: 232) loss : 1.697\n",
      "(ITER: 233) loss : 1.714\n",
      "(ITER: 234) loss : 1.706\n",
      "(ITER: 235) loss : 1.701\n",
      "(ITER: 236) loss : 1.680\n",
      "(ITER: 237) loss : 1.691\n",
      "(ITER: 238) loss : 1.694\n",
      "(ITER: 239) loss : 1.691\n",
      "(ITER: 240) loss : 1.712\n",
      "(ITER: 241) loss : 1.684\n",
      "(ITER: 242) loss : 1.687\n",
      "(ITER: 243) loss : 1.690\n",
      "(ITER: 244) loss : 1.661\n",
      "(ITER: 245) loss : 1.636\n",
      "(ITER: 246) loss : 1.664\n",
      "(ITER: 247) loss : 1.670\n",
      "(ITER: 248) loss : 1.636\n",
      "(ITER: 249) loss : 1.654\n",
      "(ITER: 250) loss : 1.666\n",
      "(ITER: 251) loss : 1.667\n",
      "(ITER: 252) loss : 1.634\n",
      "(ITER: 253) loss : 1.616\n",
      "(ITER: 254) loss : 1.621\n",
      "(ITER: 255) loss : 1.656\n",
      "(ITER: 256) loss : 1.630\n",
      "(ITER: 257) loss : 1.610\n",
      "(ITER: 258) loss : 1.604\n",
      "(ITER: 259) loss : 1.615\n",
      "(ITER: 260) loss : 1.629\n",
      "(ITER: 261) loss : 1.608\n",
      "(ITER: 262) loss : 1.579\n",
      "(ITER: 263) loss : 1.603\n",
      "(ITER: 264) loss : 1.574\n",
      "(ITER: 265) loss : 1.561\n",
      "(ITER: 266) loss : 1.591\n",
      "(ITER: 267) loss : 1.592\n",
      "(ITER: 268) loss : 1.592\n",
      "(ITER: 269) loss : 1.585\n",
      "(ITER: 270) loss : 1.574\n",
      "(ITER: 271) loss : 1.547\n",
      "(ITER: 272) loss : 1.540\n",
      "(ITER: 273) loss : 1.551\n",
      "(ITER: 274) loss : 1.600\n",
      "(ITER: 275) loss : 1.583\n",
      "(ITER: 276) loss : 1.558\n",
      "(ITER: 277) loss : 1.540\n",
      "(ITER: 278) loss : 1.536\n",
      "(ITER: 279) loss : 1.527\n",
      "(ITER: 280) loss : 1.529\n",
      "(ITER: 281) loss : 1.506\n",
      "(ITER: 282) loss : 1.556\n",
      "(ITER: 283) loss : 1.546\n",
      "(ITER: 284) loss : 1.529\n",
      "(ITER: 285) loss : 1.520\n",
      "(ITER: 286) loss : 1.531\n",
      "(ITER: 287) loss : 1.511\n",
      "(ITER: 288) loss : 1.494\n",
      "(ITER: 289) loss : 1.488\n",
      "(ITER: 290) loss : 1.500\n",
      "(ITER: 291) loss : 1.505\n",
      "(ITER: 292) loss : 1.476\n",
      "(ITER: 293) loss : 1.466\n",
      "(ITER: 294) loss : 1.476\n",
      "(ITER: 295) loss : 1.462\n",
      "(ITER: 296) loss : 1.461\n",
      "(ITER: 297) loss : 1.460\n",
      "(ITER: 298) loss : 1.470\n",
      "(ITER: 299) loss : 1.449\n",
      "(ITER: 300) loss : 1.486\n",
      "(ITER: 301) loss : 1.460\n",
      "(ITER: 302) loss : 1.445\n",
      "(ITER: 303) loss : 1.451\n",
      "(ITER: 304) loss : 1.435\n",
      "(ITER: 305) loss : 1.482\n",
      "(ITER: 306) loss : 1.468\n",
      "(ITER: 307) loss : 1.423\n",
      "(ITER: 308) loss : 1.408\n",
      "(ITER: 309) loss : 1.413\n",
      "(ITER: 310) loss : 1.419\n",
      "(ITER: 311) loss : 1.388\n",
      "(ITER: 312) loss : 1.394\n",
      "(ITER: 313) loss : 1.406\n",
      "(ITER: 314) loss : 1.426\n",
      "(ITER: 315) loss : 1.425\n",
      "(ITER: 316) loss : 1.415\n",
      "(ITER: 317) loss : 1.452\n",
      "(ITER: 318) loss : 1.383\n",
      "(ITER: 319) loss : 1.385\n",
      "(ITER: 320) loss : 1.425\n",
      "(ITER: 321) loss : 1.418\n",
      "(ITER: 322) loss : 1.391\n",
      "(ITER: 323) loss : 1.410\n",
      "(ITER: 324) loss : 1.401\n",
      "(ITER: 325) loss : 1.367\n",
      "(ITER: 326) loss : 1.383\n",
      "(ITER: 327) loss : 1.344\n",
      "(ITER: 328) loss : 1.348\n",
      "(ITER: 329) loss : 1.345\n",
      "(ITER: 330) loss : 1.353\n",
      "(ITER: 331) loss : 1.342\n",
      "(ITER: 332) loss : 1.354\n",
      "(ITER: 333) loss : 1.348\n",
      "(ITER: 334) loss : 1.340\n",
      "(ITER: 335) loss : 1.331\n",
      "(ITER: 336) loss : 1.349\n",
      "(ITER: 337) loss : 1.349\n",
      "(ITER: 338) loss : 1.339\n",
      "(ITER: 339) loss : 1.378\n",
      "(ITER: 340) loss : 1.353\n",
      "(ITER: 341) loss : 1.341\n",
      "(ITER: 342) loss : 1.307\n",
      "(ITER: 343) loss : 1.284\n",
      "(ITER: 344) loss : 1.316\n",
      "(ITER: 345) loss : 1.316\n",
      "(ITER: 346) loss : 1.296\n",
      "(ITER: 347) loss : 1.323\n",
      "(ITER: 348) loss : 1.295\n",
      "(ITER: 349) loss : 1.302\n",
      "(ITER: 350) loss : 1.327\n",
      "(ITER: 351) loss : 1.279\n",
      "(ITER: 352) loss : 1.283\n",
      "(ITER: 353) loss : 1.301\n",
      "(ITER: 354) loss : 1.323\n",
      "(ITER: 355) loss : 1.332\n",
      "(ITER: 356) loss : 1.302\n",
      "(ITER: 357) loss : 1.282\n",
      "(ITER: 358) loss : 1.285\n",
      "(ITER: 359) loss : 1.261\n",
      "(ITER: 360) loss : 1.262\n",
      "(ITER: 361) loss : 1.257\n",
      "(ITER: 362) loss : 1.242\n",
      "(ITER: 363) loss : 1.259\n",
      "(ITER: 364) loss : 1.258\n",
      "(ITER: 365) loss : 1.225\n",
      "(ITER: 366) loss : 1.255\n",
      "(ITER: 367) loss : 1.241\n",
      "(ITER: 368) loss : 1.214\n",
      "(ITER: 369) loss : 1.266\n",
      "(ITER: 370) loss : 1.252\n",
      "(ITER: 371) loss : 1.301\n",
      "(ITER: 372) loss : 1.270\n",
      "(ITER: 373) loss : 1.256\n",
      "(ITER: 374) loss : 1.276\n",
      "(ITER: 375) loss : 1.229\n",
      "(ITER: 376) loss : 1.221\n",
      "(ITER: 377) loss : 1.235\n",
      "(ITER: 378) loss : 1.233\n",
      "(ITER: 379) loss : 1.210\n",
      "(ITER: 380) loss : 1.208\n",
      "(ITER: 381) loss : 1.213\n",
      "(ITER: 382) loss : 1.229\n",
      "(ITER: 383) loss : 1.284\n",
      "(ITER: 384) loss : 1.258\n",
      "(ITER: 385) loss : 1.255\n",
      "(ITER: 386) loss : 1.184\n",
      "(ITER: 387) loss : 1.185\n",
      "(ITER: 388) loss : 1.175\n",
      "(ITER: 389) loss : 1.228\n",
      "(ITER: 390) loss : 1.212\n",
      "(ITER: 391) loss : 1.194\n",
      "(ITER: 392) loss : 1.205\n",
      "(ITER: 393) loss : 1.206\n",
      "(ITER: 394) loss : 1.217\n",
      "(ITER: 395) loss : 1.154\n",
      "(ITER: 396) loss : 1.193\n",
      "(ITER: 397) loss : 1.176\n",
      "(ITER: 398) loss : 1.186\n",
      "(ITER: 399) loss : 1.139\n",
      "(ITER: 400) loss : 1.145\n",
      "(ITER: 401) loss : 1.134\n",
      "(ITER: 402) loss : 1.142\n",
      "(ITER: 403) loss : 1.143\n",
      "(ITER: 404) loss : 1.148\n",
      "(ITER: 405) loss : 1.143\n",
      "(ITER: 406) loss : 1.217\n",
      "(ITER: 407) loss : 1.148\n",
      "(ITER: 408) loss : 1.119\n",
      "(ITER: 409) loss : 1.144\n",
      "(ITER: 410) loss : 1.148\n",
      "(ITER: 411) loss : 1.131\n",
      "(ITER: 412) loss : 1.139\n",
      "(ITER: 413) loss : 1.135\n",
      "(ITER: 414) loss : 1.120\n",
      "(ITER: 415) loss : 1.107\n",
      "(ITER: 416) loss : 1.123\n",
      "(ITER: 417) loss : 1.144\n",
      "(ITER: 418) loss : 1.146\n",
      "(ITER: 419) loss : 1.123\n",
      "(ITER: 420) loss : 1.107\n",
      "(ITER: 421) loss : 1.106\n",
      "(ITER: 422) loss : 1.101\n",
      "(ITER: 423) loss : 1.097\n",
      "(ITER: 424) loss : 1.126\n",
      "(ITER: 425) loss : 1.128\n",
      "(ITER: 426) loss : 1.072\n",
      "(ITER: 427) loss : 1.073\n",
      "(ITER: 428) loss : 1.076\n",
      "(ITER: 429) loss : 1.083\n",
      "(ITER: 430) loss : 1.052\n",
      "(ITER: 431) loss : 1.073\n",
      "(ITER: 432) loss : 1.111\n",
      "(ITER: 433) loss : 1.099\n",
      "(ITER: 434) loss : 1.087\n",
      "(ITER: 435) loss : 1.096\n",
      "(ITER: 436) loss : 1.130\n",
      "(ITER: 437) loss : 1.101\n",
      "(ITER: 438) loss : 1.094\n",
      "(ITER: 439) loss : 1.116\n",
      "(ITER: 440) loss : 1.046\n",
      "(ITER: 441) loss : 1.052\n",
      "(ITER: 442) loss : 1.051\n",
      "(ITER: 443) loss : 1.018\n",
      "(ITER: 444) loss : 1.074\n",
      "(ITER: 445) loss : 1.027\n",
      "(ITER: 446) loss : 1.050\n",
      "(ITER: 447) loss : 1.052\n",
      "(ITER: 448) loss : 1.031\n",
      "(ITER: 449) loss : 1.032\n",
      "(ITER: 450) loss : 1.035\n",
      "(ITER: 451) loss : 1.047\n",
      "(ITER: 452) loss : 1.042\n",
      "(ITER: 453) loss : 1.019\n",
      "(ITER: 454) loss : 1.042\n",
      "(ITER: 455) loss : 1.024\n",
      "(ITER: 456) loss : 1.050\n",
      "(ITER: 457) loss : 1.010\n",
      "(ITER: 458) loss : 1.015\n",
      "(ITER: 459) loss : 1.011\n",
      "(ITER: 460) loss : 1.013\n",
      "(ITER: 461) loss : 0.985\n",
      "(ITER: 462) loss : 1.030\n",
      "(ITER: 463) loss : 1.010\n",
      "(ITER: 464) loss : 1.018\n",
      "(ITER: 465) loss : 0.992\n",
      "(ITER: 466) loss : 1.003\n",
      "(ITER: 467) loss : 0.976\n",
      "(ITER: 468) loss : 1.123\n",
      "(ITER: 469) loss : 1.012\n",
      "(ITER: 470) loss : 1.054\n",
      "(ITER: 471) loss : 1.058\n",
      "(ITER: 472) loss : 1.052\n",
      "(ITER: 473) loss : 0.988\n",
      "(ITER: 474) loss : 0.993\n",
      "(ITER: 475) loss : 0.976\n",
      "(ITER: 476) loss : 0.975\n",
      "(ITER: 477) loss : 0.984\n",
      "(ITER: 478) loss : 0.998\n",
      "(ITER: 479) loss : 0.959\n",
      "(ITER: 480) loss : 0.966\n",
      "(ITER: 481) loss : 0.983\n",
      "(ITER: 482) loss : 0.953\n",
      "(ITER: 483) loss : 0.976\n",
      "(ITER: 484) loss : 1.004\n",
      "(ITER: 485) loss : 0.984\n",
      "(ITER: 486) loss : 0.997\n",
      "(ITER: 487) loss : 0.982\n",
      "(ITER: 488) loss : 0.962\n",
      "(ITER: 489) loss : 0.933\n",
      "(ITER: 490) loss : 0.965\n",
      "(ITER: 491) loss : 1.006\n",
      "(ITER: 492) loss : 0.984\n",
      "(ITER: 493) loss : 0.964\n",
      "(ITER: 494) loss : 0.937\n",
      "(ITER: 495) loss : 0.931\n",
      "(ITER: 496) loss : 0.950\n",
      "(ITER: 497) loss : 0.944\n",
      "(ITER: 498) loss : 0.945\n",
      "(ITER: 499) loss : 0.944\n",
      "(ITER: 500) loss : 0.915\n",
      "(ITER: 501) loss : 0.924\n",
      "(ITER: 502) loss : 0.943\n",
      "(ITER: 503) loss : 0.939\n",
      "(ITER: 504) loss : 0.911\n",
      "(ITER: 505) loss : 0.931\n",
      "(ITER: 506) loss : 0.932\n",
      "(ITER: 507) loss : 0.926\n",
      "(ITER: 508) loss : 0.896\n",
      "(ITER: 509) loss : 0.939\n",
      "(ITER: 510) loss : 0.919\n",
      "(ITER: 511) loss : 0.943\n",
      "(ITER: 512) loss : 0.967\n",
      "(ITER: 513) loss : 0.985\n",
      "(ITER: 514) loss : 0.940\n",
      "(ITER: 515) loss : 0.941\n",
      "(ITER: 516) loss : 0.959\n",
      "(ITER: 517) loss : 0.912\n",
      "(ITER: 518) loss : 0.907\n",
      "(ITER: 519) loss : 0.911\n",
      "(ITER: 520) loss : 0.912\n",
      "(ITER: 521) loss : 0.932\n",
      "(ITER: 522) loss : 0.889\n",
      "(ITER: 523) loss : 0.906\n",
      "(ITER: 524) loss : 0.894\n",
      "(ITER: 525) loss : 0.908\n",
      "(ITER: 526) loss : 0.886\n",
      "(ITER: 527) loss : 0.925\n",
      "(ITER: 528) loss : 0.884\n",
      "(ITER: 529) loss : 0.892\n",
      "(ITER: 530) loss : 0.895\n",
      "(ITER: 531) loss : 0.938\n",
      "(ITER: 532) loss : 0.881\n",
      "(ITER: 533) loss : 0.882\n",
      "(ITER: 534) loss : 0.862\n",
      "(ITER: 535) loss : 0.890\n",
      "(ITER: 536) loss : 0.859\n",
      "(ITER: 537) loss : 0.873\n",
      "(ITER: 538) loss : 0.873\n",
      "(ITER: 539) loss : 0.852\n",
      "(ITER: 540) loss : 0.866\n",
      "(ITER: 541) loss : 0.869\n",
      "(ITER: 542) loss : 0.881\n",
      "(ITER: 543) loss : 0.864\n",
      "(ITER: 544) loss : 0.867\n",
      "(ITER: 545) loss : 0.833\n",
      "(ITER: 546) loss : 0.843\n",
      "(ITER: 547) loss : 0.918\n",
      "(ITER: 548) loss : 1.137\n",
      "(ITER: 549) loss : 0.915\n",
      "(ITER: 550) loss : 0.945\n",
      "(ITER: 551) loss : 0.844\n",
      "(ITER: 552) loss : 0.865\n",
      "(ITER: 553) loss : 0.845\n",
      "(ITER: 554) loss : 0.845\n",
      "(ITER: 555) loss : 0.840\n",
      "(ITER: 556) loss : 0.842\n",
      "(ITER: 557) loss : 0.858\n",
      "(ITER: 558) loss : 0.858\n",
      "(ITER: 559) loss : 0.825\n",
      "(ITER: 560) loss : 0.812\n",
      "(ITER: 561) loss : 0.830\n",
      "(ITER: 562) loss : 0.838\n",
      "(ITER: 563) loss : 0.805\n",
      "(ITER: 564) loss : 0.827\n",
      "(ITER: 565) loss : 0.831\n",
      "(ITER: 566) loss : 0.821\n",
      "(ITER: 567) loss : 0.813\n",
      "(ITER: 568) loss : 0.800\n",
      "(ITER: 569) loss : 0.827\n",
      "(ITER: 570) loss : 0.831\n",
      "(ITER: 571) loss : 0.819\n",
      "(ITER: 572) loss : 0.820\n",
      "(ITER: 573) loss : 0.832\n",
      "(ITER: 574) loss : 0.811\n",
      "(ITER: 575) loss : 0.823\n",
      "(ITER: 576) loss : 0.818\n",
      "(ITER: 577) loss : 0.935\n",
      "(ITER: 578) loss : 0.846\n",
      "(ITER: 579) loss : 0.875\n",
      "(ITER: 580) loss : 0.856\n",
      "(ITER: 581) loss : 0.798\n",
      "(ITER: 582) loss : 0.824\n",
      "(ITER: 583) loss : 0.806\n",
      "(ITER: 584) loss : 0.795\n",
      "(ITER: 585) loss : 0.802\n",
      "(ITER: 586) loss : 0.810\n",
      "(ITER: 587) loss : 0.824\n",
      "(ITER: 588) loss : 0.797\n",
      "(ITER: 589) loss : 0.787\n",
      "(ITER: 590) loss : 0.846\n",
      "(ITER: 591) loss : 0.802\n",
      "(ITER: 592) loss : 0.816\n",
      "(ITER: 593) loss : 0.817\n",
      "(ITER: 594) loss : 0.782\n",
      "(ITER: 595) loss : 0.807\n",
      "(ITER: 596) loss : 0.866\n",
      "(ITER: 597) loss : 0.793\n",
      "(ITER: 598) loss : 0.801\n",
      "(ITER: 599) loss : 0.805\n",
      "(ITER: 600) loss : 0.793\n",
      "(ITER: 601) loss : 0.784\n",
      "(ITER: 602) loss : 0.784\n",
      "(ITER: 603) loss : 0.785\n",
      "(ITER: 604) loss : 0.781\n",
      "(ITER: 605) loss : 0.788\n",
      "(ITER: 606) loss : 0.788\n",
      "(ITER: 607) loss : 0.768\n",
      "(ITER: 608) loss : 0.808\n",
      "(ITER: 609) loss : 1.052\n",
      "(ITER: 610) loss : 0.892\n",
      "(ITER: 611) loss : 0.797\n",
      "(ITER: 612) loss : 0.789\n",
      "(ITER: 613) loss : 0.777\n",
      "(ITER: 614) loss : 0.807\n",
      "(ITER: 615) loss : 0.779\n",
      "(ITER: 616) loss : 0.781\n",
      "(ITER: 617) loss : 0.763\n",
      "(ITER: 618) loss : 0.779\n",
      "(ITER: 619) loss : 0.751\n",
      "(ITER: 620) loss : 0.738\n",
      "(ITER: 621) loss : 0.756\n",
      "(ITER: 622) loss : 0.757\n",
      "(ITER: 623) loss : 0.758\n",
      "(ITER: 624) loss : 0.810\n",
      "(ITER: 625) loss : 0.761\n",
      "(ITER: 626) loss : 0.787\n",
      "(ITER: 627) loss : 0.773\n",
      "(ITER: 628) loss : 0.797\n",
      "(ITER: 629) loss : 0.754\n",
      "(ITER: 630) loss : 0.733\n",
      "(ITER: 631) loss : 0.744\n",
      "(ITER: 632) loss : 0.751\n",
      "(ITER: 633) loss : 0.755\n",
      "(ITER: 634) loss : 0.755\n",
      "(ITER: 635) loss : 0.753\n",
      "(ITER: 636) loss : 0.848\n",
      "(ITER: 637) loss : 0.763\n",
      "(ITER: 638) loss : 0.780\n",
      "(ITER: 639) loss : 0.749\n",
      "(ITER: 640) loss : 0.766\n",
      "(ITER: 641) loss : 0.793\n",
      "(ITER: 642) loss : 0.767\n",
      "(ITER: 643) loss : 0.731\n",
      "(ITER: 644) loss : 0.750\n",
      "(ITER: 645) loss : 0.752\n",
      "(ITER: 646) loss : 0.797\n",
      "(ITER: 647) loss : 0.742\n",
      "(ITER: 648) loss : 0.718\n",
      "(ITER: 649) loss : 0.698\n",
      "(ITER: 650) loss : 0.741\n",
      "(ITER: 651) loss : 0.730\n",
      "(ITER: 652) loss : 0.714\n",
      "(ITER: 653) loss : 0.724\n",
      "(ITER: 654) loss : 0.717\n",
      "(ITER: 655) loss : 0.750\n",
      "(ITER: 656) loss : 0.870\n",
      "(ITER: 657) loss : 0.801\n",
      "(ITER: 658) loss : 0.788\n",
      "(ITER: 659) loss : 0.741\n",
      "(ITER: 660) loss : 0.697\n",
      "(ITER: 661) loss : 0.713\n",
      "(ITER: 662) loss : 0.728\n",
      "(ITER: 663) loss : 0.714\n",
      "(ITER: 664) loss : 0.714\n",
      "(ITER: 665) loss : 0.697\n",
      "(ITER: 666) loss : 0.742\n",
      "(ITER: 667) loss : 0.735\n",
      "(ITER: 668) loss : 0.731\n",
      "(ITER: 669) loss : 0.701\n",
      "(ITER: 670) loss : 0.792\n",
      "(ITER: 671) loss : 0.690\n",
      "(ITER: 672) loss : 0.721\n",
      "(ITER: 673) loss : 0.712\n",
      "(ITER: 674) loss : 0.733\n",
      "(ITER: 675) loss : 0.721\n",
      "(ITER: 676) loss : 0.700\n",
      "(ITER: 677) loss : 0.721\n",
      "(ITER: 678) loss : 0.737\n",
      "(ITER: 679) loss : 0.746\n",
      "(ITER: 680) loss : 0.733\n",
      "(ITER: 681) loss : 0.720\n",
      "(ITER: 682) loss : 0.710\n",
      "(ITER: 683) loss : 0.711\n",
      "(ITER: 684) loss : 0.715\n",
      "(ITER: 685) loss : 0.702\n",
      "(ITER: 686) loss : 0.709\n",
      "(ITER: 687) loss : 0.691\n",
      "(ITER: 688) loss : 0.735\n",
      "(ITER: 689) loss : 1.027\n",
      "(ITER: 690) loss : 0.772\n",
      "(ITER: 691) loss : 0.732\n",
      "(ITER: 692) loss : 0.705\n",
      "(ITER: 693) loss : 0.698\n",
      "(ITER: 694) loss : 0.731\n",
      "(ITER: 695) loss : 0.732\n",
      "(ITER: 696) loss : 0.699\n",
      "(ITER: 697) loss : 0.662\n",
      "(ITER: 698) loss : 0.689\n",
      "(ITER: 699) loss : 0.670\n",
      "(ITER: 700) loss : 0.691\n",
      "(ITER: 701) loss : 0.686\n",
      "(ITER: 702) loss : 0.677\n",
      "(ITER: 703) loss : 0.685\n",
      "(ITER: 704) loss : 0.678\n",
      "(ITER: 705) loss : 0.679\n",
      "(ITER: 706) loss : 0.668\n",
      "(ITER: 707) loss : 0.781\n",
      "(ITER: 708) loss : 0.674\n",
      "(ITER: 709) loss : 0.672\n",
      "(ITER: 710) loss : 0.674\n",
      "(ITER: 711) loss : 0.730\n",
      "(ITER: 712) loss : 0.681\n",
      "(ITER: 713) loss : 0.697\n",
      "(ITER: 714) loss : 0.665\n",
      "(ITER: 715) loss : 0.652\n",
      "(ITER: 716) loss : 0.671\n",
      "(ITER: 717) loss : 0.674\n",
      "(ITER: 718) loss : 0.659\n",
      "(ITER: 719) loss : 0.661\n",
      "(ITER: 720) loss : 0.641\n",
      "(ITER: 721) loss : 0.731\n",
      "(ITER: 722) loss : 0.873\n",
      "(ITER: 723) loss : 0.685\n",
      "(ITER: 724) loss : 0.709\n",
      "(ITER: 725) loss : 0.683\n",
      "(ITER: 726) loss : 0.705\n",
      "(ITER: 727) loss : 0.694\n",
      "(ITER: 728) loss : 0.664\n",
      "(ITER: 729) loss : 0.644\n",
      "(ITER: 730) loss : 0.645\n",
      "(ITER: 731) loss : 0.646\n",
      "(ITER: 732) loss : 0.653\n",
      "(ITER: 733) loss : 0.654\n",
      "(ITER: 734) loss : 0.638\n",
      "(ITER: 735) loss : 0.688\n",
      "(ITER: 736) loss : 0.637\n",
      "(ITER: 737) loss : 0.644\n",
      "(ITER: 738) loss : 0.639\n",
      "(ITER: 739) loss : 0.640\n",
      "(ITER: 740) loss : 0.678\n",
      "(ITER: 741) loss : 0.757\n",
      "(ITER: 742) loss : 0.646\n",
      "(ITER: 743) loss : 0.654\n",
      "(ITER: 744) loss : 0.668\n",
      "(ITER: 745) loss : 0.638\n",
      "(ITER: 746) loss : 0.732\n",
      "(ITER: 747) loss : 0.661\n",
      "(ITER: 748) loss : 0.689\n",
      "(ITER: 749) loss : 0.633\n",
      "(ITER: 750) loss : 0.629\n",
      "(ITER: 751) loss : 0.636\n",
      "(ITER: 752) loss : 0.621\n",
      "(ITER: 753) loss : 0.639\n",
      "(ITER: 754) loss : 0.682\n",
      "(ITER: 755) loss : 0.631\n",
      "(ITER: 756) loss : 0.662\n",
      "(ITER: 757) loss : 0.682\n",
      "(ITER: 758) loss : 0.634\n",
      "(ITER: 759) loss : 0.656\n",
      "(ITER: 760) loss : 0.722\n",
      "(ITER: 761) loss : 0.640\n",
      "(ITER: 762) loss : 0.633\n",
      "(ITER: 763) loss : 0.639\n",
      "(ITER: 764) loss : 0.616\n",
      "(ITER: 765) loss : 0.610\n",
      "(ITER: 766) loss : 0.598\n",
      "(ITER: 767) loss : 0.618\n",
      "(ITER: 768) loss : 0.616\n",
      "(ITER: 769) loss : 0.606\n",
      "(ITER: 770) loss : 0.633\n",
      "(ITER: 771) loss : 1.050\n",
      "(ITER: 772) loss : 0.776\n",
      "(ITER: 773) loss : 0.650\n",
      "(ITER: 774) loss : 0.653\n",
      "(ITER: 775) loss : 0.631\n",
      "(ITER: 776) loss : 0.639\n",
      "(ITER: 777) loss : 0.612\n",
      "(ITER: 778) loss : 0.624\n",
      "(ITER: 779) loss : 0.625\n",
      "(ITER: 780) loss : 0.609\n",
      "(ITER: 781) loss : 0.606\n",
      "(ITER: 782) loss : 0.615\n",
      "(ITER: 783) loss : 0.620\n",
      "(ITER: 784) loss : 0.616\n",
      "(ITER: 785) loss : 0.595\n",
      "(ITER: 786) loss : 0.594\n",
      "(ITER: 787) loss : 0.636\n",
      "(ITER: 788) loss : 0.707\n",
      "(ITER: 789) loss : 0.608\n",
      "(ITER: 790) loss : 0.648\n",
      "(ITER: 791) loss : 0.611\n",
      "(ITER: 792) loss : 0.618\n",
      "(ITER: 793) loss : 0.635\n",
      "(ITER: 794) loss : 0.694\n",
      "(ITER: 795) loss : 0.647\n",
      "(ITER: 796) loss : 0.609\n",
      "(ITER: 797) loss : 0.609\n",
      "(ITER: 798) loss : 0.624\n",
      "(ITER: 799) loss : 0.606\n",
      "(ITER: 800) loss : 0.599\n",
      "(ITER: 801) loss : 0.602\n",
      "(ITER: 802) loss : 0.646\n",
      "(ITER: 803) loss : 0.851\n",
      "(ITER: 804) loss : 0.696\n",
      "(ITER: 805) loss : 0.610\n",
      "(ITER: 806) loss : 0.599\n",
      "(ITER: 807) loss : 0.615\n",
      "(ITER: 808) loss : 0.619\n",
      "(ITER: 809) loss : 0.586\n",
      "(ITER: 810) loss : 0.602\n",
      "(ITER: 811) loss : 0.609\n",
      "(ITER: 812) loss : 0.592\n",
      "(ITER: 813) loss : 0.609\n",
      "(ITER: 814) loss : 0.645\n",
      "(ITER: 815) loss : 0.595\n",
      "(ITER: 816) loss : 0.603\n",
      "(ITER: 817) loss : 0.605\n",
      "(ITER: 818) loss : 0.660\n",
      "(ITER: 819) loss : 0.606\n",
      "(ITER: 820) loss : 0.616\n",
      "(ITER: 821) loss : 0.594\n",
      "(ITER: 822) loss : 0.601\n",
      "(ITER: 823) loss : 0.699\n",
      "(ITER: 824) loss : 0.579\n",
      "(ITER: 825) loss : 0.596\n",
      "(ITER: 826) loss : 0.608\n",
      "(ITER: 827) loss : 0.593\n",
      "(ITER: 828) loss : 0.571\n",
      "(ITER: 829) loss : 0.579\n",
      "(ITER: 830) loss : 0.591\n",
      "(ITER: 831) loss : 0.576\n",
      "(ITER: 832) loss : 0.579\n",
      "(ITER: 833) loss : 0.820\n",
      "(ITER: 834) loss : 0.649\n",
      "(ITER: 835) loss : 0.679\n",
      "(ITER: 836) loss : 0.587\n",
      "(ITER: 837) loss : 0.611\n",
      "(ITER: 838) loss : 0.579\n",
      "(ITER: 839) loss : 0.567\n",
      "(ITER: 840) loss : 0.581\n",
      "(ITER: 841) loss : 0.572\n",
      "(ITER: 842) loss : 0.591\n",
      "(ITER: 843) loss : 0.564\n",
      "(ITER: 844) loss : 0.629\n",
      "(ITER: 845) loss : 0.628\n",
      "(ITER: 846) loss : 0.630\n",
      "(ITER: 847) loss : 0.602\n",
      "(ITER: 848) loss : 0.573\n",
      "(ITER: 849) loss : 0.567\n",
      "(ITER: 850) loss : 0.566\n",
      "(ITER: 851) loss : 0.562\n",
      "(ITER: 852) loss : 0.578\n",
      "(ITER: 853) loss : 0.641\n",
      "(ITER: 854) loss : 0.590\n",
      "(ITER: 855) loss : 0.596\n",
      "(ITER: 856) loss : 0.561\n",
      "(ITER: 857) loss : 0.564\n",
      "(ITER: 858) loss : 0.556\n",
      "(ITER: 859) loss : 0.593\n",
      "(ITER: 860) loss : 0.654\n",
      "(ITER: 861) loss : 0.574\n",
      "(ITER: 862) loss : 0.596\n",
      "(ITER: 863) loss : 0.562\n",
      "(ITER: 864) loss : 0.582\n",
      "(ITER: 865) loss : 0.782\n",
      "(ITER: 866) loss : 0.600\n",
      "(ITER: 867) loss : 0.592\n",
      "(ITER: 868) loss : 0.574\n",
      "(ITER: 869) loss : 0.574\n",
      "(ITER: 870) loss : 0.643\n",
      "(ITER: 871) loss : 0.564\n",
      "(ITER: 872) loss : 0.566\n",
      "(ITER: 873) loss : 0.571\n",
      "(ITER: 874) loss : 0.562\n",
      "(ITER: 875) loss : 0.652\n",
      "(ITER: 876) loss : 0.587\n",
      "(ITER: 877) loss : 0.556\n",
      "(ITER: 878) loss : 0.581\n",
      "(ITER: 879) loss : 0.572\n",
      "(ITER: 880) loss : 0.586\n",
      "(ITER: 881) loss : 0.562\n",
      "(ITER: 882) loss : 0.559\n",
      "(ITER: 883) loss : 0.551\n",
      "(ITER: 884) loss : 0.611\n",
      "(ITER: 885) loss : 0.578\n",
      "(ITER: 886) loss : 0.570\n",
      "(ITER: 887) loss : 0.549\n",
      "(ITER: 888) loss : 0.576\n",
      "(ITER: 889) loss : 0.577\n",
      "(ITER: 890) loss : 0.575\n",
      "(ITER: 891) loss : 0.580\n",
      "(ITER: 892) loss : 0.567\n",
      "(ITER: 893) loss : 0.563\n",
      "(ITER: 894) loss : 0.557\n",
      "(ITER: 895) loss : 0.578\n",
      "(ITER: 896) loss : 0.601\n",
      "(ITER: 897) loss : 0.570\n",
      "(ITER: 898) loss : 0.545\n",
      "(ITER: 899) loss : 0.570\n",
      "(ITER: 900) loss : 0.573\n",
      "(ITER: 901) loss : 0.548\n",
      "(ITER: 902) loss : 0.569\n",
      "(ITER: 903) loss : 0.527\n",
      "(ITER: 904) loss : 0.536\n",
      "(ITER: 905) loss : 0.569\n",
      "(ITER: 906) loss : 0.698\n",
      "(ITER: 907) loss : 0.551\n",
      "(ITER: 908) loss : 0.545\n",
      "(ITER: 909) loss : 0.549\n",
      "(ITER: 910) loss : 0.540\n",
      "(ITER: 911) loss : 0.602\n",
      "(ITER: 912) loss : 0.576\n",
      "(ITER: 913) loss : 0.569\n",
      "(ITER: 914) loss : 0.544\n",
      "(ITER: 915) loss : 0.538\n",
      "(ITER: 916) loss : 0.531\n",
      "(ITER: 917) loss : 0.540\n",
      "(ITER: 918) loss : 0.545\n",
      "(ITER: 919) loss : 0.536\n",
      "(ITER: 920) loss : 0.764\n",
      "(ITER: 921) loss : 0.566\n",
      "(ITER: 922) loss : 0.637\n",
      "(ITER: 923) loss : 0.571\n",
      "(ITER: 924) loss : 0.584\n",
      "(ITER: 925) loss : 0.549\n",
      "(ITER: 926) loss : 0.553\n",
      "(ITER: 927) loss : 0.563\n",
      "(ITER: 928) loss : 0.561\n",
      "(ITER: 929) loss : 0.525\n",
      "(ITER: 930) loss : 0.549\n",
      "(ITER: 931) loss : 0.532\n",
      "(ITER: 932) loss : 0.568\n",
      "(ITER: 933) loss : 0.548\n",
      "(ITER: 934) loss : 0.566\n",
      "(ITER: 935) loss : 0.521\n",
      "(ITER: 936) loss : 0.536\n",
      "(ITER: 937) loss : 0.525\n",
      "(ITER: 938) loss : 0.544\n",
      "(ITER: 939) loss : 0.649\n",
      "(ITER: 940) loss : 0.540\n",
      "(ITER: 941) loss : 0.521\n",
      "(ITER: 942) loss : 0.544\n",
      "(ITER: 943) loss : 0.540\n",
      "(ITER: 944) loss : 0.533\n",
      "(ITER: 945) loss : 0.525\n",
      "(ITER: 946) loss : 0.566\n",
      "(ITER: 947) loss : 0.540\n",
      "(ITER: 948) loss : 0.562\n",
      "(ITER: 949) loss : 0.578\n",
      "(ITER: 950) loss : 0.522\n",
      "(ITER: 951) loss : 0.551\n",
      "(ITER: 952) loss : 0.594\n",
      "(ITER: 953) loss : 0.528\n",
      "(ITER: 954) loss : 0.522\n",
      "(ITER: 955) loss : 0.565\n",
      "(ITER: 956) loss : 0.656\n",
      "(ITER: 957) loss : 0.524\n",
      "(ITER: 958) loss : 0.544\n",
      "(ITER: 959) loss : 0.568\n",
      "(ITER: 960) loss : 0.532\n",
      "(ITER: 961) loss : 0.530\n",
      "(ITER: 962) loss : 0.594\n",
      "(ITER: 963) loss : 0.556\n",
      "(ITER: 964) loss : 0.563\n",
      "(ITER: 965) loss : 0.530\n",
      "(ITER: 966) loss : 0.562\n",
      "(ITER: 967) loss : 0.536\n",
      "(ITER: 968) loss : 0.543\n",
      "(ITER: 969) loss : 0.529\n",
      "(ITER: 970) loss : 0.528\n",
      "(ITER: 971) loss : 0.527\n",
      "(ITER: 972) loss : 0.549\n",
      "(ITER: 973) loss : 0.604\n",
      "(ITER: 974) loss : 0.535\n",
      "(ITER: 975) loss : 0.544\n",
      "(ITER: 976) loss : 0.557\n",
      "(ITER: 977) loss : 0.524\n",
      "(ITER: 978) loss : 0.526\n",
      "(ITER: 979) loss : 0.540\n",
      "(ITER: 980) loss : 0.525\n",
      "(ITER: 981) loss : 0.516\n",
      "(ITER: 982) loss : 0.652\n",
      "(ITER: 983) loss : 0.558\n",
      "(ITER: 984) loss : 0.559\n",
      "(ITER: 985) loss : 0.541\n",
      "(ITER: 986) loss : 0.617\n",
      "(ITER: 987) loss : 0.606\n",
      "(ITER: 988) loss : 0.566\n",
      "(ITER: 989) loss : 0.557\n",
      "(ITER: 990) loss : 0.541\n",
      "(ITER: 991) loss : 0.545\n",
      "(ITER: 992) loss : 0.532\n",
      "(ITER: 993) loss : 0.525\n",
      "(ITER: 994) loss : 0.501\n",
      "(ITER: 995) loss : 0.512\n",
      "(ITER: 996) loss : 0.522\n",
      "(ITER: 997) loss : 0.498\n",
      "(ITER: 998) loss : 0.525\n",
      "(ITER: 999) loss : 0.506\n",
      "(ITER: 1000) loss : 0.518\n",
      "(ITER: 1001) loss : 0.553\n",
      "(ITER: 1002) loss : 0.617\n",
      "(ITER: 1003) loss : 0.509\n",
      "(ITER: 1004) loss : 0.512\n",
      "(ITER: 1005) loss : 0.636\n",
      "(ITER: 1006) loss : 0.528\n",
      "(ITER: 1007) loss : 0.512\n",
      "(ITER: 1008) loss : 0.565\n",
      "(ITER: 1009) loss : 0.563\n",
      "(ITER: 1010) loss : 0.522\n",
      "(ITER: 1011) loss : 0.500\n",
      "(ITER: 1012) loss : 0.490\n",
      "(ITER: 1013) loss : 0.510\n",
      "(ITER: 1014) loss : 0.533\n",
      "(ITER: 1015) loss : 0.546\n",
      "(ITER: 1016) loss : 0.496\n",
      "(ITER: 1017) loss : 0.504\n",
      "(ITER: 1018) loss : 0.511\n",
      "(ITER: 1019) loss : 0.692\n",
      "(ITER: 1020) loss : 0.538\n",
      "(ITER: 1021) loss : 0.625\n",
      "(ITER: 1022) loss : 0.567\n",
      "(ITER: 1023) loss : 0.555\n",
      "(ITER: 1024) loss : 0.537\n",
      "(ITER: 1025) loss : 0.505\n",
      "(ITER: 1026) loss : 0.501\n",
      "(ITER: 1027) loss : 0.492\n",
      "(ITER: 1028) loss : 0.490\n",
      "(ITER: 1029) loss : 0.514\n",
      "(ITER: 1030) loss : 0.498\n",
      "(ITER: 1031) loss : 0.515\n",
      "(ITER: 1032) loss : 0.516\n",
      "(ITER: 1033) loss : 0.493\n",
      "(ITER: 1034) loss : 0.516\n",
      "(ITER: 1035) loss : 0.503\n",
      "(ITER: 1036) loss : 0.507\n",
      "(ITER: 1037) loss : 0.542\n",
      "(ITER: 1038) loss : 0.508\n",
      "(ITER: 1039) loss : 0.553\n",
      "(ITER: 1040) loss : 0.506\n",
      "(ITER: 1041) loss : 0.491\n",
      "(ITER: 1042) loss : 0.512\n",
      "(ITER: 1043) loss : 0.494\n",
      "(ITER: 1044) loss : 0.522\n",
      "(ITER: 1045) loss : 0.582\n",
      "(ITER: 1046) loss : 0.560\n",
      "(ITER: 1047) loss : 0.538\n",
      "(ITER: 1048) loss : 0.541\n",
      "(ITER: 1049) loss : 0.504\n",
      "(ITER: 1050) loss : 0.511\n",
      "(ITER: 1051) loss : 0.511\n",
      "(ITER: 1052) loss : 0.495\n",
      "(ITER: 1053) loss : 0.506\n",
      "(ITER: 1054) loss : 0.494\n",
      "(ITER: 1055) loss : 0.480\n",
      "(ITER: 1056) loss : 0.512\n",
      "(ITER: 1057) loss : 0.507\n",
      "(ITER: 1058) loss : 0.499\n",
      "(ITER: 1059) loss : 0.497\n",
      "(ITER: 1060) loss : 0.671\n",
      "(ITER: 1061) loss : 0.513\n",
      "(ITER: 1062) loss : 0.509\n",
      "(ITER: 1063) loss : 0.493\n",
      "(ITER: 1064) loss : 0.537\n",
      "(ITER: 1065) loss : 0.506\n",
      "(ITER: 1066) loss : 0.498\n",
      "(ITER: 1067) loss : 0.517\n",
      "(ITER: 1068) loss : 0.469\n",
      "(ITER: 1069) loss : 0.510\n",
      "(ITER: 1070) loss : 0.567\n",
      "(ITER: 1071) loss : 0.497\n",
      "(ITER: 1072) loss : 0.496\n",
      "(ITER: 1073) loss : 0.484\n",
      "(ITER: 1074) loss : 0.504\n",
      "(ITER: 1075) loss : 0.484\n",
      "(ITER: 1076) loss : 0.500\n",
      "(ITER: 1077) loss : 0.516\n",
      "(ITER: 1078) loss : 0.498\n",
      "(ITER: 1079) loss : 0.468\n",
      "(ITER: 1080) loss : 0.556\n",
      "(ITER: 1081) loss : 0.830\n",
      "(ITER: 1082) loss : 0.527\n",
      "(ITER: 1083) loss : 0.517\n",
      "(ITER: 1084) loss : 0.516\n",
      "(ITER: 1085) loss : 0.493\n",
      "(ITER: 1086) loss : 0.523\n",
      "(ITER: 1087) loss : 0.499\n",
      "(ITER: 1088) loss : 0.494\n",
      "(ITER: 1089) loss : 0.465\n",
      "(ITER: 1090) loss : 0.478\n",
      "(ITER: 1091) loss : 0.466\n",
      "(ITER: 1092) loss : 0.478\n",
      "(ITER: 1093) loss : 0.499\n",
      "(ITER: 1094) loss : 0.474\n",
      "(ITER: 1095) loss : 0.510\n",
      "(ITER: 1096) loss : 0.521\n",
      "(ITER: 1097) loss : 0.514\n",
      "(ITER: 1098) loss : 0.481\n",
      "(ITER: 1099) loss : 0.491\n",
      "(ITER: 1100) loss : 0.512\n",
      "(ITER: 1101) loss : 0.510\n",
      "(ITER: 1102) loss : 0.492\n",
      "(ITER: 1103) loss : 0.520\n",
      "(ITER: 1104) loss : 0.560\n",
      "(ITER: 1105) loss : 0.506\n",
      "(ITER: 1106) loss : 0.499\n",
      "(ITER: 1107) loss : 0.495\n",
      "(ITER: 1108) loss : 0.678\n",
      "(ITER: 1109) loss : 0.505\n",
      "(ITER: 1110) loss : 0.577\n",
      "(ITER: 1111) loss : 0.483\n",
      "(ITER: 1112) loss : 0.499\n",
      "(ITER: 1113) loss : 0.491\n",
      "(ITER: 1114) loss : 0.479\n",
      "(ITER: 1115) loss : 0.475\n",
      "(ITER: 1116) loss : 0.484\n",
      "(ITER: 1117) loss : 0.468\n",
      "(ITER: 1118) loss : 0.482\n",
      "(ITER: 1119) loss : 0.464\n",
      "(ITER: 1120) loss : 0.466\n",
      "(ITER: 1121) loss : 0.458\n",
      "(ITER: 1122) loss : 0.492\n",
      "(ITER: 1123) loss : 0.553\n",
      "(ITER: 1124) loss : 0.479\n",
      "(ITER: 1125) loss : 0.479\n",
      "(ITER: 1126) loss : 0.469\n",
      "(ITER: 1127) loss : 0.475\n",
      "(ITER: 1128) loss : 0.487\n",
      "(ITER: 1129) loss : 0.465\n",
      "(ITER: 1130) loss : 0.482\n",
      "(ITER: 1131) loss : 0.665\n",
      "(ITER: 1132) loss : 0.476\n",
      "(ITER: 1133) loss : 0.557\n",
      "(ITER: 1134) loss : 0.544\n",
      "(ITER: 1135) loss : 0.552\n",
      "(ITER: 1136) loss : 0.477\n",
      "(ITER: 1137) loss : 0.466\n",
      "(ITER: 1138) loss : 0.465\n",
      "(ITER: 1139) loss : 0.450\n",
      "(ITER: 1140) loss : 0.452\n",
      "(ITER: 1141) loss : 0.473\n",
      "(ITER: 1142) loss : 0.475\n",
      "(ITER: 1143) loss : 0.482\n",
      "(ITER: 1144) loss : 0.458\n",
      "(ITER: 1145) loss : 0.471\n",
      "(ITER: 1146) loss : 0.486\n",
      "(ITER: 1147) loss : 0.518\n",
      "(ITER: 1148) loss : 0.469\n",
      "(ITER: 1149) loss : 0.485\n",
      "(ITER: 1150) loss : 0.461\n",
      "(ITER: 1151) loss : 0.443\n",
      "(ITER: 1152) loss : 0.494\n",
      "(ITER: 1153) loss : 0.709\n",
      "(ITER: 1154) loss : 0.516\n",
      "(ITER: 1155) loss : 0.523\n",
      "(ITER: 1156) loss : 0.520\n",
      "(ITER: 1157) loss : 0.470\n",
      "(ITER: 1158) loss : 0.483\n",
      "(ITER: 1159) loss : 0.461\n",
      "(ITER: 1160) loss : 0.448\n",
      "(ITER: 1161) loss : 0.478\n",
      "(ITER: 1162) loss : 0.457\n",
      "(ITER: 1163) loss : 0.455\n",
      "(ITER: 1164) loss : 0.458\n",
      "(ITER: 1165) loss : 0.476\n",
      "(ITER: 1166) loss : 0.462\n",
      "(ITER: 1167) loss : 0.450\n",
      "(ITER: 1168) loss : 0.446\n",
      "(ITER: 1169) loss : 0.558\n",
      "(ITER: 1170) loss : 0.765\n",
      "(ITER: 1171) loss : 0.528\n",
      "(ITER: 1172) loss : 0.533\n",
      "(ITER: 1173) loss : 0.522\n",
      "(ITER: 1174) loss : 0.468\n",
      "(ITER: 1175) loss : 0.468\n",
      "(ITER: 1176) loss : 0.460\n",
      "(ITER: 1177) loss : 0.447\n",
      "(ITER: 1178) loss : 0.453\n",
      "(ITER: 1179) loss : 0.457\n",
      "(ITER: 1180) loss : 0.457\n",
      "(ITER: 1181) loss : 0.445\n",
      "(ITER: 1182) loss : 0.452\n",
      "(ITER: 1183) loss : 0.488\n",
      "(ITER: 1184) loss : 0.492\n",
      "(ITER: 1185) loss : 0.469\n",
      "(ITER: 1186) loss : 0.483\n",
      "(ITER: 1187) loss : 0.480\n",
      "(ITER: 1188) loss : 0.463\n",
      "(ITER: 1189) loss : 0.457\n",
      "(ITER: 1190) loss : 0.489\n",
      "(ITER: 1191) loss : 0.542\n",
      "(ITER: 1192) loss : 0.513\n",
      "(ITER: 1193) loss : 0.485\n",
      "(ITER: 1194) loss : 0.483\n",
      "(ITER: 1195) loss : 0.471\n",
      "(ITER: 1196) loss : 0.448\n",
      "(ITER: 1197) loss : 0.478\n",
      "(ITER: 1198) loss : 0.470\n",
      "(ITER: 1199) loss : 0.467\n",
      "(ITER: 1200) loss : 0.463\n",
      "(ITER: 1201) loss : 0.474\n",
      "(ITER: 1202) loss : 0.463\n",
      "(ITER: 1203) loss : 0.444\n",
      "(ITER: 1204) loss : 0.438\n",
      "(ITER: 1205) loss : 0.467\n",
      "(ITER: 1206) loss : 0.466\n",
      "(ITER: 1207) loss : 0.552\n",
      "(ITER: 1208) loss : 0.623\n",
      "(ITER: 1209) loss : 0.491\n",
      "(ITER: 1210) loss : 0.458\n",
      "(ITER: 1211) loss : 0.480\n",
      "(ITER: 1212) loss : 0.449\n",
      "(ITER: 1213) loss : 0.453\n",
      "(ITER: 1214) loss : 0.455\n",
      "(ITER: 1215) loss : 0.440\n",
      "(ITER: 1216) loss : 0.449\n",
      "(ITER: 1217) loss : 0.456\n",
      "(ITER: 1218) loss : 0.457\n",
      "(ITER: 1219) loss : 0.505\n",
      "(ITER: 1220) loss : 0.439\n",
      "(ITER: 1221) loss : 0.462\n",
      "(ITER: 1222) loss : 0.444\n",
      "(ITER: 1223) loss : 0.490\n",
      "(ITER: 1224) loss : 0.560\n",
      "(ITER: 1225) loss : 0.469\n",
      "(ITER: 1226) loss : 0.476\n",
      "(ITER: 1227) loss : 0.492\n",
      "(ITER: 1228) loss : 0.564\n",
      "(ITER: 1229) loss : 0.471\n",
      "(ITER: 1230) loss : 0.446\n",
      "(ITER: 1231) loss : 0.437\n",
      "(ITER: 1232) loss : 0.450\n",
      "(ITER: 1233) loss : 0.563\n",
      "(ITER: 1234) loss : 0.498\n",
      "(ITER: 1235) loss : 0.484\n",
      "(ITER: 1236) loss : 0.500\n",
      "(ITER: 1237) loss : 0.480\n",
      "(ITER: 1238) loss : 0.469\n",
      "(ITER: 1239) loss : 0.467\n",
      "(ITER: 1240) loss : 0.453\n",
      "(ITER: 1241) loss : 0.462\n",
      "(ITER: 1242) loss : 0.468\n",
      "(ITER: 1243) loss : 0.519\n",
      "(ITER: 1244) loss : 0.452\n",
      "(ITER: 1245) loss : 0.458\n",
      "(ITER: 1246) loss : 0.442\n",
      "(ITER: 1247) loss : 0.464\n",
      "(ITER: 1248) loss : 0.521\n",
      "(ITER: 1249) loss : 0.492\n",
      "(ITER: 1250) loss : 0.474\n",
      "(ITER: 1251) loss : 0.468\n",
      "(ITER: 1252) loss : 0.478\n",
      "(ITER: 1253) loss : 0.437\n",
      "(ITER: 1254) loss : 0.449\n",
      "(ITER: 1255) loss : 0.439\n",
      "(ITER: 1256) loss : 0.467\n",
      "(ITER: 1257) loss : 0.519\n",
      "(ITER: 1258) loss : 0.440\n",
      "(ITER: 1259) loss : 0.455\n",
      "(ITER: 1260) loss : 0.460\n",
      "(ITER: 1261) loss : 0.520\n",
      "(ITER: 1262) loss : 0.501\n",
      "(ITER: 1263) loss : 0.468\n",
      "(ITER: 1264) loss : 0.448\n",
      "(ITER: 1265) loss : 0.445\n",
      "(ITER: 1266) loss : 0.438\n",
      "(ITER: 1267) loss : 0.437\n",
      "(ITER: 1268) loss : 0.463\n",
      "(ITER: 1269) loss : 0.482\n",
      "(ITER: 1270) loss : 0.448\n",
      "(ITER: 1271) loss : 0.436\n",
      "(ITER: 1272) loss : 0.467\n",
      "(ITER: 1273) loss : 0.757\n",
      "(ITER: 1274) loss : 0.550\n",
      "(ITER: 1275) loss : 0.445\n",
      "(ITER: 1276) loss : 0.467\n",
      "(ITER: 1277) loss : 0.472\n",
      "(ITER: 1278) loss : 0.451\n",
      "(ITER: 1279) loss : 0.436\n",
      "(ITER: 1280) loss : 0.427\n",
      "(ITER: 1281) loss : 0.423\n",
      "(ITER: 1282) loss : 0.427\n",
      "(ITER: 1283) loss : 0.436\n",
      "(ITER: 1284) loss : 0.419\n",
      "(ITER: 1285) loss : 0.435\n",
      "(ITER: 1286) loss : 0.431\n",
      "(ITER: 1287) loss : 0.434\n",
      "(ITER: 1288) loss : 0.616\n",
      "(ITER: 1289) loss : 0.488\n",
      "(ITER: 1290) loss : 0.557\n",
      "(ITER: 1291) loss : 0.437\n",
      "(ITER: 1292) loss : 0.486\n",
      "(ITER: 1293) loss : 0.451\n",
      "(ITER: 1294) loss : 0.437\n",
      "(ITER: 1295) loss : 0.434\n",
      "(ITER: 1296) loss : 0.426\n",
      "(ITER: 1297) loss : 0.431\n",
      "(ITER: 1298) loss : 0.427\n",
      "(ITER: 1299) loss : 0.434\n",
      "(ITER: 1300) loss : 0.423\n",
      "(ITER: 1301) loss : 0.443\n",
      "(ITER: 1302) loss : 0.416\n",
      "(ITER: 1303) loss : 0.427\n",
      "(ITER: 1304) loss : 0.436\n",
      "(ITER: 1305) loss : 0.500\n",
      "(ITER: 1306) loss : 0.805\n",
      "(ITER: 1307) loss : 0.496\n",
      "(ITER: 1308) loss : 0.486\n",
      "(ITER: 1309) loss : 0.460\n",
      "(ITER: 1310) loss : 0.491\n",
      "(ITER: 1311) loss : 0.473\n",
      "(ITER: 1312) loss : 0.429\n",
      "(ITER: 1313) loss : 0.425\n",
      "(ITER: 1314) loss : 0.424\n",
      "(ITER: 1315) loss : 0.442\n",
      "(ITER: 1316) loss : 0.450\n",
      "(ITER: 1317) loss : 0.433\n",
      "(ITER: 1318) loss : 0.438\n",
      "(ITER: 1319) loss : 0.431\n",
      "(ITER: 1320) loss : 0.421\n",
      "(ITER: 1321) loss : 0.470\n",
      "(ITER: 1322) loss : 0.511\n",
      "(ITER: 1323) loss : 0.528\n",
      "(ITER: 1324) loss : 0.434\n",
      "(ITER: 1325) loss : 0.440\n",
      "(ITER: 1326) loss : 0.517\n",
      "(ITER: 1327) loss : 0.485\n",
      "(ITER: 1328) loss : 0.429\n",
      "(ITER: 1329) loss : 0.427\n",
      "(ITER: 1330) loss : 0.455\n",
      "(ITER: 1331) loss : 0.513\n",
      "(ITER: 1332) loss : 0.420\n",
      "(ITER: 1333) loss : 0.445\n",
      "(ITER: 1334) loss : 0.454\n",
      "(ITER: 1335) loss : 0.545\n",
      "(ITER: 1336) loss : 0.445\n",
      "(ITER: 1337) loss : 0.458\n",
      "(ITER: 1338) loss : 0.458\n",
      "(ITER: 1339) loss : 0.477\n",
      "(ITER: 1340) loss : 0.456\n",
      "(ITER: 1341) loss : 0.439\n",
      "(ITER: 1342) loss : 0.458\n",
      "(ITER: 1343) loss : 0.455\n",
      "(ITER: 1344) loss : 0.454\n",
      "(ITER: 1345) loss : 0.434\n",
      "(ITER: 1346) loss : 0.419\n",
      "(ITER: 1347) loss : 0.448\n",
      "(ITER: 1348) loss : 0.577\n",
      "(ITER: 1349) loss : 0.436\n",
      "(ITER: 1350) loss : 0.424\n",
      "(ITER: 1351) loss : 0.444\n",
      "(ITER: 1352) loss : 0.446\n",
      "(ITER: 1353) loss : 0.449\n",
      "(ITER: 1354) loss : 0.421\n",
      "(ITER: 1355) loss : 0.416\n",
      "(ITER: 1356) loss : 0.432\n",
      "(ITER: 1357) loss : 0.416\n",
      "(ITER: 1358) loss : 0.427\n",
      "(ITER: 1359) loss : 0.419\n",
      "(ITER: 1360) loss : 0.431\n",
      "(ITER: 1361) loss : 0.572\n",
      "(ITER: 1362) loss : 0.492\n",
      "(ITER: 1363) loss : 0.442\n",
      "(ITER: 1364) loss : 0.489\n",
      "(ITER: 1365) loss : 0.455\n",
      "(ITER: 1366) loss : 0.462\n",
      "(ITER: 1367) loss : 0.450\n",
      "(ITER: 1368) loss : 0.418\n",
      "(ITER: 1369) loss : 0.420\n",
      "(ITER: 1370) loss : 0.426\n",
      "(ITER: 1371) loss : 0.412\n",
      "(ITER: 1372) loss : 0.437\n",
      "(ITER: 1373) loss : 0.417\n",
      "(ITER: 1374) loss : 0.431\n",
      "(ITER: 1375) loss : 0.447\n",
      "(ITER: 1376) loss : 0.415\n",
      "(ITER: 1377) loss : 0.422\n",
      "(ITER: 1378) loss : 0.424\n",
      "(ITER: 1379) loss : 0.587\n",
      "(ITER: 1380) loss : 0.459\n",
      "(ITER: 1381) loss : 0.495\n",
      "(ITER: 1382) loss : 0.491\n",
      "(ITER: 1383) loss : 0.491\n",
      "(ITER: 1384) loss : 0.432\n",
      "(ITER: 1385) loss : 0.413\n",
      "(ITER: 1386) loss : 0.418\n",
      "(ITER: 1387) loss : 0.413\n",
      "(ITER: 1388) loss : 0.411\n",
      "(ITER: 1389) loss : 0.423\n",
      "(ITER: 1390) loss : 0.431\n",
      "(ITER: 1391) loss : 0.443\n",
      "(ITER: 1392) loss : 0.431\n",
      "(ITER: 1393) loss : 0.454\n",
      "(ITER: 1394) loss : 0.584\n",
      "(ITER: 1395) loss : 0.466\n",
      "(ITER: 1396) loss : 0.439\n",
      "(ITER: 1397) loss : 0.448\n",
      "(ITER: 1398) loss : 0.444\n",
      "(ITER: 1399) loss : 0.441\n",
      "(ITER: 1400) loss : 0.428\n",
      "(ITER: 1401) loss : 0.433\n",
      "(ITER: 1402) loss : 0.449\n",
      "(ITER: 1403) loss : 0.446\n",
      "(ITER: 1404) loss : 0.500\n",
      "(ITER: 1405) loss : 0.565\n",
      "(ITER: 1406) loss : 0.469\n",
      "(ITER: 1407) loss : 0.428\n",
      "(ITER: 1408) loss : 0.417\n",
      "(ITER: 1409) loss : 0.428\n",
      "(ITER: 1410) loss : 0.414\n",
      "(ITER: 1411) loss : 0.417\n",
      "(ITER: 1412) loss : 0.410\n",
      "(ITER: 1413) loss : 0.406\n",
      "(ITER: 1414) loss : 0.412\n",
      "(ITER: 1415) loss : 0.406\n",
      "(ITER: 1416) loss : 0.419\n",
      "(ITER: 1417) loss : 0.543\n",
      "(ITER: 1418) loss : 0.554\n",
      "(ITER: 1419) loss : 0.435\n",
      "(ITER: 1420) loss : 0.427\n",
      "(ITER: 1421) loss : 0.425\n",
      "(ITER: 1422) loss : 0.430\n",
      "(ITER: 1423) loss : 0.424\n",
      "(ITER: 1424) loss : 0.441\n",
      "(ITER: 1425) loss : 0.426\n",
      "(ITER: 1426) loss : 0.409\n",
      "(ITER: 1427) loss : 0.420\n",
      "(ITER: 1428) loss : 0.460\n",
      "(ITER: 1429) loss : 0.528\n",
      "(ITER: 1430) loss : 0.485\n",
      "(ITER: 1431) loss : 0.432\n",
      "(ITER: 1432) loss : 0.441\n",
      "(ITER: 1433) loss : 0.431\n",
      "(ITER: 1434) loss : 0.423\n",
      "(ITER: 1435) loss : 0.411\n",
      "(ITER: 1436) loss : 0.422\n",
      "(ITER: 1437) loss : 0.430\n",
      "(ITER: 1438) loss : 0.438\n",
      "(ITER: 1439) loss : 0.555\n",
      "(ITER: 1440) loss : 0.434\n",
      "(ITER: 1441) loss : 0.419\n",
      "(ITER: 1442) loss : 0.463\n",
      "(ITER: 1443) loss : 0.527\n",
      "(ITER: 1444) loss : 0.419\n",
      "(ITER: 1445) loss : 0.423\n",
      "(ITER: 1446) loss : 0.423\n",
      "(ITER: 1447) loss : 0.433\n",
      "(ITER: 1448) loss : 0.424\n",
      "(ITER: 1449) loss : 0.412\n",
      "(ITER: 1450) loss : 0.409\n",
      "(ITER: 1451) loss : 0.447\n",
      "(ITER: 1452) loss : 0.664\n",
      "(ITER: 1453) loss : 0.519\n",
      "(ITER: 1454) loss : 0.485\n",
      "(ITER: 1455) loss : 0.446\n",
      "(ITER: 1456) loss : 0.427\n",
      "(ITER: 1457) loss : 0.425\n",
      "(ITER: 1458) loss : 0.397\n",
      "(ITER: 1459) loss : 0.422\n",
      "(ITER: 1460) loss : 0.407\n",
      "(ITER: 1461) loss : 0.399\n",
      "(ITER: 1462) loss : 0.426\n",
      "(ITER: 1463) loss : 0.398\n",
      "(ITER: 1464) loss : 0.527\n",
      "(ITER: 1465) loss : 0.441\n",
      "(ITER: 1466) loss : 0.435\n",
      "(ITER: 1467) loss : 0.457\n",
      "(ITER: 1468) loss : 0.410\n",
      "(ITER: 1469) loss : 0.406\n",
      "(ITER: 1470) loss : 0.411\n",
      "(ITER: 1471) loss : 0.465\n",
      "(ITER: 1472) loss : 0.498\n",
      "(ITER: 1473) loss : 0.424\n",
      "(ITER: 1474) loss : 0.439\n",
      "(ITER: 1475) loss : 0.412\n",
      "(ITER: 1476) loss : 0.455\n",
      "(ITER: 1477) loss : 0.508\n",
      "(ITER: 1478) loss : 0.468\n",
      "(ITER: 1479) loss : 0.436\n",
      "(ITER: 1480) loss : 0.408\n",
      "(ITER: 1481) loss : 0.412\n",
      "(ITER: 1482) loss : 0.415\n",
      "(ITER: 1483) loss : 0.424\n",
      "(ITER: 1484) loss : 0.414\n",
      "(ITER: 1485) loss : 0.398\n",
      "(ITER: 1486) loss : 0.425\n",
      "(ITER: 1487) loss : 0.743\n",
      "(ITER: 1488) loss : 0.591\n",
      "(ITER: 1489) loss : 0.521\n",
      "(ITER: 1490) loss : 0.464\n",
      "(ITER: 1491) loss : 0.485\n",
      "(ITER: 1492) loss : 0.411\n",
      "(ITER: 1493) loss : 0.417\n",
      "(ITER: 1494) loss : 0.421\n",
      "(ITER: 1495) loss : 0.416\n",
      "(ITER: 1496) loss : 0.401\n",
      "(ITER: 1497) loss : 0.392\n",
      "(ITER: 1498) loss : 0.396\n",
      "(ITER: 1499) loss : 0.406\n",
      "(ITER: 1500) loss : 0.406\n",
      "(ITER: 1501) loss : 0.403\n",
      "(ITER: 1502) loss : 0.448\n",
      "(ITER: 1503) loss : 0.451\n",
      "(ITER: 1504) loss : 0.410\n",
      "(ITER: 1505) loss : 0.418\n",
      "(ITER: 1506) loss : 0.390\n",
      "(ITER: 1507) loss : 0.414\n",
      "(ITER: 1508) loss : 0.516\n",
      "(ITER: 1509) loss : 0.408\n",
      "(ITER: 1510) loss : 0.430\n",
      "(ITER: 1511) loss : 0.408\n",
      "(ITER: 1512) loss : 0.449\n",
      "(ITER: 1513) loss : 0.541\n",
      "(ITER: 1514) loss : 0.446\n",
      "(ITER: 1515) loss : 0.434\n",
      "(ITER: 1516) loss : 0.408\n",
      "(ITER: 1517) loss : 0.438\n",
      "(ITER: 1518) loss : 0.479\n",
      "(ITER: 1519) loss : 0.424\n",
      "(ITER: 1520) loss : 0.393\n",
      "(ITER: 1521) loss : 0.416\n",
      "(ITER: 1522) loss : 0.426\n",
      "(ITER: 1523) loss : 0.440\n",
      "(ITER: 1524) loss : 0.425\n",
      "(ITER: 1525) loss : 0.406\n",
      "(ITER: 1526) loss : 0.411\n",
      "(ITER: 1527) loss : 0.408\n",
      "(ITER: 1528) loss : 0.396\n",
      "(ITER: 1529) loss : 0.421\n",
      "(ITER: 1530) loss : 0.587\n",
      "(ITER: 1531) loss : 0.412\n",
      "(ITER: 1532) loss : 0.443\n",
      "(ITER: 1533) loss : 0.446\n",
      "(ITER: 1534) loss : 0.433\n",
      "(ITER: 1535) loss : 0.411\n",
      "(ITER: 1536) loss : 0.420\n",
      "(ITER: 1537) loss : 0.413\n",
      "(ITER: 1538) loss : 0.390\n",
      "(ITER: 1539) loss : 0.383\n",
      "(ITER: 1540) loss : 0.385\n",
      "(ITER: 1541) loss : 0.396\n",
      "(ITER: 1542) loss : 0.416\n",
      "(ITER: 1543) loss : 0.575\n",
      "(ITER: 1544) loss : 0.420\n",
      "(ITER: 1545) loss : 0.399\n",
      "(ITER: 1546) loss : 0.406\n",
      "(ITER: 1547) loss : 0.444\n",
      "(ITER: 1548) loss : 0.438\n",
      "(ITER: 1549) loss : 0.412\n",
      "(ITER: 1550) loss : 0.398\n",
      "(ITER: 1551) loss : 0.418\n",
      "(ITER: 1552) loss : 0.397\n",
      "(ITER: 1553) loss : 0.388\n",
      "(ITER: 1554) loss : 0.422\n",
      "(ITER: 1555) loss : 0.433\n",
      "(ITER: 1556) loss : 0.461\n",
      "(ITER: 1557) loss : 0.448\n",
      "(ITER: 1558) loss : 0.422\n",
      "(ITER: 1559) loss : 0.402\n",
      "(ITER: 1560) loss : 0.409\n",
      "(ITER: 1561) loss : 0.579\n",
      "(ITER: 1562) loss : 0.568\n",
      "(ITER: 1563) loss : 0.469\n",
      "(ITER: 1564) loss : 0.451\n",
      "(ITER: 1565) loss : 0.453\n",
      "(ITER: 1566) loss : 0.402\n",
      "(ITER: 1567) loss : 0.393\n",
      "(ITER: 1568) loss : 0.406\n",
      "(ITER: 1569) loss : 0.389\n",
      "(ITER: 1570) loss : 0.391\n",
      "(ITER: 1571) loss : 0.392\n",
      "(ITER: 1572) loss : 0.428\n",
      "(ITER: 1573) loss : 0.469\n",
      "(ITER: 1574) loss : 0.415\n",
      "(ITER: 1575) loss : 0.426\n",
      "(ITER: 1576) loss : 0.410\n",
      "(ITER: 1577) loss : 0.393\n",
      "(ITER: 1578) loss : 0.384\n",
      "(ITER: 1579) loss : 0.413\n",
      "(ITER: 1580) loss : 0.446\n",
      "(ITER: 1581) loss : 0.440\n",
      "(ITER: 1582) loss : 0.407\n",
      "(ITER: 1583) loss : 0.405\n",
      "(ITER: 1584) loss : 0.409\n",
      "(ITER: 1585) loss : 0.743\n",
      "(ITER: 1586) loss : 0.605\n",
      "(ITER: 1587) loss : 0.514\n",
      "(ITER: 1588) loss : 0.405\n",
      "(ITER: 1589) loss : 0.452\n",
      "(ITER: 1590) loss : 0.422\n",
      "(ITER: 1591) loss : 0.407\n",
      "(ITER: 1592) loss : 0.393\n",
      "(ITER: 1593) loss : 0.398\n",
      "(ITER: 1594) loss : 0.423\n",
      "(ITER: 1595) loss : 0.394\n",
      "(ITER: 1596) loss : 0.392\n",
      "(ITER: 1597) loss : 0.382\n",
      "(ITER: 1598) loss : 0.378\n",
      "(ITER: 1599) loss : 0.376\n",
      "(ITER: 1600) loss : 0.381\n",
      "(ITER: 1601) loss : 0.443\n",
      "(ITER: 1602) loss : 0.839\n",
      "(ITER: 1603) loss : 0.507\n",
      "(ITER: 1604) loss : 0.429\n",
      "(ITER: 1605) loss : 0.434\n",
      "(ITER: 1606) loss : 0.426\n",
      "(ITER: 1607) loss : 0.411\n",
      "(ITER: 1608) loss : 0.397\n",
      "(ITER: 1609) loss : 0.383\n",
      "(ITER: 1610) loss : 0.379\n",
      "(ITER: 1611) loss : 0.387\n",
      "(ITER: 1612) loss : 0.386\n",
      "(ITER: 1613) loss : 0.381\n",
      "(ITER: 1614) loss : 0.388\n",
      "(ITER: 1615) loss : 0.386\n",
      "(ITER: 1616) loss : 0.383\n",
      "(ITER: 1617) loss : 0.399\n",
      "(ITER: 1618) loss : 0.439\n",
      "(ITER: 1619) loss : 0.575\n",
      "(ITER: 1620) loss : 0.446\n",
      "(ITER: 1621) loss : 0.405\n",
      "(ITER: 1622) loss : 0.393\n",
      "(ITER: 1623) loss : 0.426\n",
      "(ITER: 1624) loss : 0.451\n",
      "(ITER: 1625) loss : 0.393\n",
      "(ITER: 1626) loss : 0.381\n",
      "(ITER: 1627) loss : 0.407\n",
      "(ITER: 1628) loss : 0.397\n",
      "(ITER: 1629) loss : 0.380\n",
      "(ITER: 1630) loss : 0.390\n",
      "(ITER: 1631) loss : 0.428\n",
      "(ITER: 1632) loss : 0.442\n",
      "(ITER: 1633) loss : 0.427\n",
      "(ITER: 1634) loss : 0.406\n",
      "(ITER: 1635) loss : 0.387\n",
      "(ITER: 1636) loss : 0.379\n",
      "(ITER: 1637) loss : 0.402\n",
      "(ITER: 1638) loss : 0.411\n",
      "(ITER: 1639) loss : 0.496\n",
      "(ITER: 1640) loss : 0.506\n",
      "(ITER: 1641) loss : 0.441\n",
      "(ITER: 1642) loss : 0.420\n",
      "(ITER: 1643) loss : 0.385\n",
      "(ITER: 1644) loss : 0.410\n",
      "(ITER: 1645) loss : 0.378\n",
      "(ITER: 1646) loss : 0.384\n",
      "(ITER: 1647) loss : 0.393\n",
      "(ITER: 1648) loss : 0.404\n",
      "(ITER: 1649) loss : 0.387\n",
      "(ITER: 1650) loss : 0.382\n",
      "(ITER: 1651) loss : 0.422\n",
      "(ITER: 1652) loss : 0.584\n",
      "(ITER: 1653) loss : 0.405\n",
      "(ITER: 1654) loss : 0.412\n",
      "(ITER: 1655) loss : 0.399\n",
      "(ITER: 1656) loss : 0.411\n",
      "(ITER: 1657) loss : 0.392\n",
      "(ITER: 1658) loss : 0.391\n",
      "(ITER: 1659) loss : 0.382\n",
      "(ITER: 1660) loss : 0.391\n",
      "(ITER: 1661) loss : 0.385\n",
      "(ITER: 1662) loss : 0.440\n",
      "(ITER: 1663) loss : 0.547\n",
      "(ITER: 1664) loss : 0.434\n",
      "(ITER: 1665) loss : 0.388\n",
      "(ITER: 1666) loss : 0.410\n",
      "(ITER: 1667) loss : 0.396\n",
      "(ITER: 1668) loss : 0.450\n",
      "(ITER: 1669) loss : 0.401\n",
      "(ITER: 1670) loss : 0.401\n",
      "(ITER: 1671) loss : 0.378\n",
      "(ITER: 1672) loss : 0.424\n",
      "(ITER: 1673) loss : 0.544\n",
      "(ITER: 1674) loss : 0.418\n",
      "(ITER: 1675) loss : 0.415\n",
      "(ITER: 1676) loss : 0.390\n",
      "(ITER: 1677) loss : 0.384\n",
      "(ITER: 1678) loss : 0.419\n",
      "(ITER: 1679) loss : 0.409\n",
      "(ITER: 1680) loss : 0.402\n",
      "(ITER: 1681) loss : 0.394\n",
      "(ITER: 1682) loss : 0.393\n",
      "(ITER: 1683) loss : 0.392\n",
      "(ITER: 1684) loss : 0.517\n",
      "(ITER: 1685) loss : 0.401\n",
      "(ITER: 1686) loss : 0.388\n",
      "(ITER: 1687) loss : 0.405\n",
      "(ITER: 1688) loss : 0.395\n",
      "(ITER: 1689) loss : 0.381\n",
      "(ITER: 1690) loss : 0.380\n",
      "(ITER: 1691) loss : 0.392\n",
      "(ITER: 1692) loss : 0.446\n",
      "(ITER: 1693) loss : 0.557\n",
      "(ITER: 1694) loss : 0.418\n",
      "(ITER: 1695) loss : 0.382\n",
      "(ITER: 1696) loss : 0.436\n",
      "(ITER: 1697) loss : 0.391\n",
      "(ITER: 1698) loss : 0.378\n",
      "(ITER: 1699) loss : 0.385\n",
      "(ITER: 1700) loss : 0.380\n",
      "(ITER: 1701) loss : 0.414\n",
      "(ITER: 1702) loss : 0.407\n",
      "(ITER: 1703) loss : 0.492\n",
      "(ITER: 1704) loss : 0.383\n",
      "(ITER: 1705) loss : 0.380\n",
      "(ITER: 1706) loss : 0.390\n",
      "(ITER: 1707) loss : 0.377\n",
      "(ITER: 1708) loss : 0.382\n",
      "(ITER: 1709) loss : 0.374\n",
      "(ITER: 1710) loss : 0.379\n",
      "(ITER: 1711) loss : 0.449\n",
      "(ITER: 1712) loss : 0.899\n",
      "(ITER: 1713) loss : 0.518\n",
      "(ITER: 1714) loss : 0.468\n",
      "(ITER: 1715) loss : 0.443\n",
      "(ITER: 1716) loss : 0.406\n",
      "(ITER: 1717) loss : 0.401\n",
      "(ITER: 1718) loss : 0.389\n",
      "(ITER: 1719) loss : 0.379\n",
      "(ITER: 1720) loss : 0.370\n",
      "(ITER: 1721) loss : 0.382\n",
      "(ITER: 1722) loss : 0.366\n",
      "(ITER: 1723) loss : 0.374\n",
      "(ITER: 1724) loss : 0.397\n",
      "(ITER: 1725) loss : 0.366\n",
      "(ITER: 1726) loss : 0.385\n",
      "(ITER: 1727) loss : 0.408\n",
      "(ITER: 1728) loss : 0.424\n",
      "(ITER: 1729) loss : 0.421\n",
      "(ITER: 1730) loss : 0.396\n",
      "(ITER: 1731) loss : 0.391\n",
      "(ITER: 1732) loss : 0.430\n",
      "(ITER: 1733) loss : 0.507\n",
      "(ITER: 1734) loss : 0.395\n",
      "(ITER: 1735) loss : 0.383\n",
      "(ITER: 1736) loss : 0.379\n",
      "(ITER: 1737) loss : 0.391\n",
      "(ITER: 1738) loss : 0.445\n",
      "(ITER: 1739) loss : 0.369\n",
      "(ITER: 1740) loss : 0.376\n",
      "(ITER: 1741) loss : 0.389\n",
      "(ITER: 1742) loss : 0.418\n",
      "(ITER: 1743) loss : 0.482\n",
      "(ITER: 1744) loss : 0.415\n",
      "(ITER: 1745) loss : 0.382\n",
      "(ITER: 1746) loss : 0.397\n",
      "(ITER: 1747) loss : 0.550\n",
      "(ITER: 1748) loss : 0.409\n",
      "(ITER: 1749) loss : 0.411\n",
      "(ITER: 1750) loss : 0.391\n",
      "(ITER: 1751) loss : 0.405\n",
      "(ITER: 1752) loss : 0.389\n",
      "(ITER: 1753) loss : 0.388\n",
      "(ITER: 1754) loss : 0.380\n",
      "(ITER: 1755) loss : 0.378\n",
      "(ITER: 1756) loss : 0.364\n",
      "(ITER: 1757) loss : 0.387\n",
      "(ITER: 1758) loss : 0.393\n",
      "(ITER: 1759) loss : 0.399\n",
      "(ITER: 1760) loss : 0.388\n",
      "(ITER: 1761) loss : 0.386\n",
      "(ITER: 1762) loss : 0.403\n",
      "(ITER: 1763) loss : 0.558\n",
      "(ITER: 1764) loss : 0.392\n",
      "(ITER: 1765) loss : 0.402\n",
      "(ITER: 1766) loss : 0.385\n",
      "(ITER: 1767) loss : 0.436\n",
      "(ITER: 1768) loss : 0.485\n",
      "(ITER: 1769) loss : 0.428\n",
      "(ITER: 1770) loss : 0.399\n",
      "(ITER: 1771) loss : 0.378\n",
      "(ITER: 1772) loss : 0.404\n",
      "(ITER: 1773) loss : 0.381\n",
      "(ITER: 1774) loss : 0.370\n",
      "(ITER: 1775) loss : 0.372\n",
      "(ITER: 1776) loss : 0.372\n",
      "(ITER: 1777) loss : 0.380\n",
      "(ITER: 1778) loss : 0.391\n",
      "(ITER: 1779) loss : 0.408\n",
      "(ITER: 1780) loss : 0.422\n",
      "(ITER: 1781) loss : 0.404\n",
      "(ITER: 1782) loss : 0.491\n",
      "(ITER: 1783) loss : 0.386\n",
      "(ITER: 1784) loss : 0.379\n",
      "(ITER: 1785) loss : 0.380\n",
      "(ITER: 1786) loss : 0.405\n",
      "(ITER: 1787) loss : 0.507\n",
      "(ITER: 1788) loss : 0.393\n",
      "(ITER: 1789) loss : 0.371\n",
      "(ITER: 1790) loss : 0.439\n",
      "(ITER: 1791) loss : 0.353\n",
      "(ITER: 1792) loss : 0.388\n",
      "(ITER: 1793) loss : 0.357\n",
      "(ITER: 1794) loss : 0.378\n",
      "(ITER: 1795) loss : 0.392\n",
      "(ITER: 1796) loss : 0.417\n",
      "(ITER: 1797) loss : 0.395\n",
      "(ITER: 1798) loss : 0.536\n",
      "(ITER: 1799) loss : 0.381\n"
     ]
    }
   ],
   "source": [
    "# matching\n",
    "# matching_function = KNN_GreedyMatching(k=int(np.sqrt(SM_true.shape[0]))).knn_greedy_matching   # hungarian, blossom_max_weight_matching, greedy_matching, KNN_GreedyMatching(k=10).knn_greedy_matching\n",
    "\n",
    "N_UPDATES = 5\n",
    "matching_function = greedy_matching\n",
    "for i in range(1800):\n",
    "    # user vector / id\n",
    "    users_vec = torch.LongTensor( np.stack([user.user_vec for user in users]) )\n",
    "    users_id = torch.LongTensor( np.stack([[user.id] for user in users]) )\n",
    "    \n",
    "    # forward : mu, U, Lambda\n",
    "    mu_Hat, U_Hat, Lambda_Hat = TS_model(users_vec, users_id) # Gradient\n",
    "    \n",
    "    # sampling \n",
    "    compatibility_samples = TS_model.sampling(mu_Hat, U_Hat).squeeze(-2)\n",
    "    compatibility_samples_np = compatibility_samples.to('cpu').numpy()\n",
    "    \n",
    "    # calculate similarity matrix\n",
    "    SM_samples = compatibility_samples_np @ compatibility_samples_np.T\n",
    "\n",
    "    # users matching\n",
    "    sample_matching, sample_matching_values = matching_function(SM_samples)\n",
    "    \n",
    "    # obseve rewards\n",
    "    rewards_obs = revealed_reward(SM_true, sample_matching, noise_std=0.2)\n",
    "    rewards_obs_list.append(np.sum(rewards_obs))\n",
    "    \n",
    "    # calculate_targets    \n",
    "    mu_targets, Sigma_targets = calculate_targets(rewards_obs, users, compatibility_samples_np, sample_matching, inplace=True)\n",
    "    mu_targets_tensor = torch.FloatTensor(np.stack(mu_targets))\n",
    "    Sigma_targets_tensor = torch.FloatTensor(np.stack(Sigma_targets))\n",
    "    \n",
    "    # TS-Model Update (KL-divergence Loss)\n",
    "    loss = kl_gaussian_prec_chol(mu_targets_tensor, Sigma_targets_tensor, mu_Hat, U_Hat, reduction='mean')\n",
    "    # loss = kl_gaussian_full(mu_targets_tensor, Sigma_targets_tensor,\n",
    "    #                         mu_Hat, torch.linalg.inv(Lambda_Hat), reduction='mean')\n",
    "    \n",
    "    # model update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"(ITER: {i}) loss : {loss.detach().to('cpu').numpy():.3f}\")\n",
    "    \n",
    "    for j in range(N_UPDATES-1):\n",
    "        mu_Hat, U_Hat, Lambda_Hat = TS_model(users_vec, users_id) # Gradient\n",
    "        loss = kl_gaussian_prec_chol(mu_targets_tensor, Sigma_targets_tensor, mu_Hat, U_Hat, reduction='mean')\n",
    "        # loss = kl_gaussian_full(mu_targets_tensor, Sigma_targets_tensor,\n",
    "        #                     mu_Hat, torch.linalg.inv(Lambda_Hat), reduction='mean')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf5f7851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards_obs_list)\n",
    "plt.axhline(np.sum(true_matching_values), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6eba4f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< sampling_matching >\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      "  48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      "  72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      "  96 97 98 99]\n",
      " [11 12 18 22 97 78  8 99  6 49 70  0  1 36 54 73 86 63  2 43 92 23  3 21\n",
      "  66 27 30 25 81 56 26 74 82 69 85 98 13 58 95 87 41 40 62 19 94 51 48 67\n",
      "  46  9 89 45 71 64 14 57 29 55 37 84 96 80 42 17 53 68 24 47 65 33 10 52\n",
      "  90 15 31 83 79 91  5 76 61 28 32 75 59 34 16 39 93 50 72 77 20 88 44 38\n",
      "  60  4 35  7]]\n",
      "[ 0.22074938  0.07507519 -0.00874328  0.34194788  0.57681775  0.51625603\n",
      "  0.200157    0.56475264  0.200157    0.3630356   0.28922862  0.22074938\n",
      "  0.07507519  0.31974962  0.41111198  0.2770125   0.32779703  0.3471201\n",
      " -0.00874328  0.36555383  0.14819779  0.33236575  0.34194788  0.33236575\n",
      "  0.40798974  0.20404987  0.27017966  0.20404987  0.00424372  0.24131589\n",
      "  0.27017966  0.32202357  0.33171862  0.58071816  0.21120353  0.37328663\n",
      "  0.31974962  0.35463834  0.5260632   0.37962234  0.25843126  0.25843126\n",
      "  0.42068294  0.36555383  0.56474996  0.27443632  0.30128595  0.42813274\n",
      "  0.30128595  0.3630356   0.40238473  0.27443632  0.42967132  0.10223633\n",
      "  0.41111198  0.4524674   0.24131589  0.4524674   0.35463834  0.57295036\n",
      "  0.36752886  0.08382095  0.42068294  0.3471201   0.10223633  0.49302068\n",
      "  0.40798974  0.42813274  0.49302068  0.58071816  0.28922862  0.42967132\n",
      "  0.35428974  0.2770125   0.32202357  0.08104001  0.4003031   0.51536417\n",
      "  0.51625603  0.4003031   0.08382095  0.00424372  0.33171862  0.08104001\n",
      "  0.57295036  0.21120353  0.32779703  0.37962234  0.5399359   0.40238473\n",
      "  0.35428974  0.51536417  0.14819779  0.5399359   0.56474996  0.5260632\n",
      "  0.36752886  0.57681775  0.37328663  0.56475264]\n",
      "total return rewards (training) : 33.836\n"
     ]
    }
   ],
   "source": [
    "# sample matching\n",
    "users_vec = torch.LongTensor( np.stack([user.user_vec for user in users]) )\n",
    "users_id = torch.LongTensor( np.stack([[user.id] for user in users]) )\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu_Hat, U_Hat, Lambda_Hat = TS_model(users_vec, users_id) # Gradient\n",
    "\n",
    "SM_samples = mu_Hat.numpy() @ mu_Hat.numpy().T\n",
    "\n",
    "# sample_matching, sample_matching_values = hungarian(SM_samples)\n",
    "sample_matching, sample_matching_values = blossom_max_weight_matching(SM_samples)\n",
    "print('< sampling_matching >')\n",
    "print(sample_matching)\n",
    "print(sample_matching_values)\n",
    "print(f\"total return rewards (training) : {sample_matching_values.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f1ab6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< true_matching >\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      "  48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      "  72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      "  96 97 98 99]\n",
      " [66 12 75 27 97 78 32 91 46 13 20 15  1  9 25 11 22 82 56 61 10 23 16 21\n",
      "  87 14 81  3 50 80 99 74  6 69 84 77 53 58 95 49 83 73 62 67 94 51  8 63\n",
      "  86 39 28 45 71 36 92 57 18 55 37 85 96 19 42 47 70 68  0 43 65 33 64 52\n",
      "  90 41 31  2 79 35  5 76 29 26 17 40 34 59 48 24 93 98 72  7 54 88 44 38\n",
      "  60  4 89 30]]\n",
      "[0.31706449 0.12613176 0.2586174  0.29204334 0.58486178 0.51581587\n",
      " 0.23446946 0.60495221 0.36157614 0.38401057 0.26247353 0.44821427\n",
      " 0.12613176 0.38401057 0.38288849 0.44821427 0.35907654 0.39309183\n",
      " 0.2857028  0.31786953 0.26247353 0.34687454 0.35907654 0.34687454\n",
      " 0.45511708 0.38288849 0.32313501 0.29204334 0.38084125 0.3858602\n",
      " 0.5090078  0.32746659 0.23446946 0.58180718 0.45004826 0.47036315\n",
      " 0.26481879 0.36498532 0.5273215  0.35824886 0.2668614  0.45506998\n",
      " 0.42600992 0.39046112 0.56824532 0.30695747 0.36157614 0.47685757\n",
      " 0.34139303 0.35824886 0.38084125 0.30695747 0.43918289 0.26481879\n",
      " 0.29852723 0.45367018 0.2857028  0.45367018 0.36498532 0.3399464\n",
      " 0.37099927 0.31786953 0.42600992 0.47685757 0.2974511  0.50009157\n",
      " 0.31706449 0.39046112 0.50009157 0.58180718 0.2974511  0.43918289\n",
      " 0.36115652 0.45506998 0.32746659 0.2586174  0.40675226 0.47036315\n",
      " 0.51581587 0.40675226 0.3858602  0.32313501 0.39309183 0.2668614\n",
      " 0.45004826 0.3399464  0.34139303 0.45511708 0.54531969 0.47427804\n",
      " 0.36115652 0.60495221 0.29852723 0.54531969 0.56824532 0.5273215\n",
      " 0.37099927 0.58486178 0.47427804 0.5090078 ]\n",
      "total return rewards (true) : 39.188\n"
     ]
    }
   ],
   "source": [
    "print('< true_matching >')\n",
    "print(true_matching)\n",
    "print(true_matching_values)\n",
    "print(f\"total return rewards (true) : {true_matching_values.sum():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
