import sys
sys.path.append('d:\\Python\\★★Python_POSTECH_AI\\DS_Module')    # 모듈 경로 추가
from DS_DataFrame import DS_DF_Summary, DS_OneHotEncoder, DS_LabelEncoder
from DS_OLS import *

absolute_path = 'D:/Python/★★Python_POSTECH_AI/Postech_AI 4) Aritificial_Intelligent/교재_실습_자료/'
# absolute_path = 'D:/Python/★★Python_POSTECH_AI/Dataset_AI/DataMining/'

# ------------------------------------------------------------------------------------------------------------
# 1. Markov Chain
# 1.1. Sequential Processes
# 1.2. Markov Process   
# 1.2.1. State Transition Matrix ---------------------------------------------

import numpy as np
P = [[0, 0, 1],
    [1/2, 1/2, 0],
    [1/3, 2/3, 0]]

print(P[1][:])
np_P = np.array(P)
np_P.sum(1)

np.linalg.eig(np_P)[0]
np.linalg.eig(np_P)[1][0]

a = np.random.choice(3,1, p = P[1][:])      # 2번 State에서 출발
# np.random.choice(3,1, p = P[1][:])
# 3 : range(3)
# 1 : 추출할 갯수
# p : 각각의 확률
print(a)



# sequential processes
# sequence generated by Markov chain
# S1 = 0, S2 = 1, S3 = 2

# starting from 0
x = 0
S = []
S.append(x)

for i in range(5):    
    x = np.random.choice(3,1,p = P[x][:])[0]    
    S.append(x)
     
print(S)




# 1.2.2. Markov Chain Components ---------------------------------------------
# 1.2.3. Example of Markov Chain ---------------------------------------------
P = [[0, 0.5, 0, 0, 0, 0.5, 0],
    [0, 0, 0.8, 0, 0, 0, 0.2],
    [0, 0, 0, 0.6, 0.4, 0, 0],
    [0, 0, 0, 0, 0, 0, 1],
    [0.2, 0.4, 0.4, 0, 0, 0, 0],
    [0.1, 0, 0, 0, 0, 0.9, 0],
    [0, 0, 0, 0, 0, 0, 1]]

# sequential processes
# sequence generated by Markov chain
# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]

name = ['C1','C2','C3','Pass','Pub','FB','Sleep']

# starting from 0
x = 0 
S = []
S.append(x)

for i in range(10):
    x = np.random.choice(len(P),1,p = P[x][:])[0]    
    S.append(x)
    
# print(S)
episode = list(map(lambda x: name[x], S))
print(episode)



# episode = []
# for i in S:
#     episode.append(name[i])

# print(episode)


# 1.3. Chapman-Kolmogorov Equation -----------------------------------------------------------
# state probability distribution

P = [[0, 0, 1],
    [1/2, 1/2, 0],
    [1/3, 2/3, 0]]  # p matrix

u = [0.2, 0.1, 0.7]   # 초기 확률
# u = [0.2, 0.8, 0]   # 초기 확률
# u = [0, 1, 0]

P = np.asmatrix(P)
u = np.asmatrix(u)

for i in range(10):
    u = u*P
    print(u) 


u = [0, 1, 0]
u = u*P**10
print(u)


# 1.4. Stationary Distribution ------------------------------------------------------
# eigenvalue = 1 and associated eigenvector
    # https://brilliant.org/wiki/stationary-distributions/

d, v = np.linalg.eig(P.T)
print(d) # loof for eigenvalue = 1

print((v[:,2]/np.sum(v[:,2])).ravel())        # stationary Probability

















# ------------------------------------------------------------------------------------------------------------
# 2. Markov Reward Process(MRP) ------------------------------------------------------
# 2.1. Definition of MRP ------------------------------------------------------
# 2.2. Reward over Multiple Transitions (= Return) -----------------------------------


# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]
name = ['C1','C2','C3','Pass','Pub','FB','Sleep']
R = [-2, -2, -2, 10, 1, -1, 0]
gamma = 0.9

# if a sequence is given
S = [0, 1, 2, 4, 2, 4]
S_name = list(map(lambda x: name[x], S))
S_name

# 각 시나리오에 대한 전체 Gain
def gain_total(reward, sinario, gamma):
    G = 0
    for i, state in enumerate(sinario):
        G = G + (gamma**i) * reward[state]
    return G
gain_total(reward=R, sinario=S, gamma=gamma)

# 각 시나리오에 대한 전체 Gain
G = 0
for i, state in enumerate(S):
    G = G + (gamma**i) * R[state]
G

print(G) 









R = [-2, -2, -2, 10, 1, -1, 0]
gamma = 0.9

P = [[0, 0.5, 0, 0, 0, 0.5, 0],
    [0, 0, 0.8, 0, 0, 0, 0.2],
    [0, 0, 0, 0.6, 0.4, 0, 0],
    [0, 0, 0, 0, 0, 0, 1],
    [0.2, 0.4, 0.4, 0, 0, 0, 0],
    [0.1, 0, 0, 0, 0, 0.9, 0],
    [0, 0, 0, 0, 0, 0, 1]]

# sequence generated by Markov chain
# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]

# starting from 0
x = 0 
S = []
S.append(x)

# Sinario Sequence 생성
for i in range(5):
    x = np.random.choice(len(P),1,p = P[x][:])[0]    
    S.append(x)

S_name = list(map(lambda x: name[x], S))
print(S_name)

# gain_total(reward=R, sinario=S, gamma=0.9)

G = 0
for i in range(len(S)):
    G = G + (gamma**i)*R[S[i]]
print(S)
print(G)






# 2.3. Value Function ------------------------------------------------------


# 2.4. Bellman Equations for MRP (Value Iteration) -------------------------

# 2.4.1. Bellman Equation in Matrix Form -----------------------------------

# 2.4.2. Solving the Bellman Equation --------------------------------------
# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]

P = [[0, 0.5, 0, 0, 0, 0.5, 0],
    [0, 0, 0.8, 0, 0, 0, 0.2],
    [0, 0, 0, 0.6, 0.4, 0, 0],
    [0, 0, 0, 0, 0, 0, 1],
    [0.2, 0.4, 0.4, 0, 0, 0, 0],
    [0.1, 0, 0, 0, 0, 0.9, 0],
    [0, 0, 0, 0, 0, 0, 1]]

R = [-2, -2, -2, 10, 1, -1, 0]

P = np.asmatrix(P)
R = np.asmatrix(R)
R = R.T

# Bellman Equation : directly
gamma = 0.9
v = (np.eye(7) - gamma*P).I*R
print(v)



# Bellman Equation : fixed point iteration
gamma = 0.9
v = np.zeros([7,1])

for i in range(100):
    v = R + gamma*P*v

print(v)






# Bellman Equation : fixed point iteration
gamma = 1
v = np.zeros([7,1])

for i in range(100):
    v = R + gamma*P*v

print(v)


























# ------------------------------------------------------------------------------------------------------------
# 3. Markov Decision Process ------------------------------------------------------
# 3.1. MDP Definition
# 3.2. Policies
# 3.2.1. Value Function 
# 3.2.2. Bellman Expectation Equation
# 3.2.3. Two examples


# 특정 Policy에서의 value vector
import numpy as np

# [PU PF RU RF] = [S, A, S, A]
# [PU PF RU RF] = [0 1 2 3]      # 확정된 policy
P = [[1, 0, 0, 0],
    [0, 1, 0, 0],
    [0.5, 0, 0.5, 0],
    [0, 1, 0, 0]]

R = [0, 0, 10, 10]

P = np.asmatrix(P)
R = np.asmatrix(R)
R = R.T

gamma = 0.9

v = np.zeros([4,1])
# v = [0] * 4

for _ in range(100):
    v = R + gamma * P * v

print(v)





# [PU PF RU RF] = [0 1 2 3]
# [PU PF RU RF] = [A, A ,A ,A]      # 확정된 policy
import numpy as np

P = [[0.5, 0.5, 0, 0],
    [0, 1, 0, 0],
    [0.5, 0.5, 0, 0],
    [0, 1, 0, 0]]

R = [0, 0, 10, 10]

P = np.asmatrix(P)
R = np.asmatrix(R)
R = R.T

gamma = 0.9

v = np.zeros([4,1])
# v = [0] * 4

for _ in range(100):
    v = R + gamma * P * v

print(v)



# 3.3. Optimal Policy

# 3.4.Value iteration

# (action) 0 : Save Money,  1 : Advertising

# [PU PF RU RF] = [0 1 2 3]
P = {
 0: {0: [(1, 0)], 1: [(0.5, 0), (0.5, 1)]},
 1: {0: [(0.5, 0), (0.5, 3)], 1: [(1, 1)]},
 2: {0: [(0.5, 0), (0.5, 2)], 1: [(0.5, 0), (0.5, 1)]},
 3: {0: [(0.5, 2), (0.5, 3)], 1: [(1, 1)]},
}

R = [0, 0, 10, 10]
gamma = 0.9

States = [0, 1, 2, 3]       # 가능한 state
Actions = [0, 1]            # 가능한 action


# state =2, action = 0 일때,
v = [0, 0, 10, 10]      # value_function이 주어졌을때
P[2][0]


# (MRP) v = R + P·v
# P·v 부분만 계산
reward = 0
for trans in P[2][0]:
    reward = reward + trans[0] * v[trans[1]]
print(reward)



# 전체에 대하여 # optimal value fuction
v = np.zeros(4)
v_next = np.zeros(4)

# v* = R + γ·max(∑P·v*)
for i in range(100):
    for s in States:
        q_0 = sum(trans[0] * v[trans[1]] for trans in P[s][0])  # save money시 얻는 보상 기대값
        q_1 = sum(trans[0] * v[trans[1]] for trans in P[s][1])  # advertising시 얻는 보상 기대값

        v_next[s] = R[s] + gamma * max(q_0, q_1)       # v = R + γ·P·v
    v = v_next.copy()       # 모든 state에 대해 수행후에 value를 업데이트
print(v)    # state 0에 있을때 기대되는 value

sum([trans[0] * v[trans[1]] for trans in P[s][0]])
sum([trans[0] * v[trans[1]] for trans in P[s][1]])


# optimal policy
# once v computed
optPolicy = [0]*4       # value_function을 수렴시킨후에 policy를 계산

for s in States:
    q_0 = sum(trans[0]*v[trans[1]] for trans in P[s][0])
    q_1 = sum(trans[0]*v[trans[1]] for trans in P[s][1])   
    
    optPolicy[s] = np.argmax([q_0, q_1])

print(optPolicy) 



# shorten (위의 코드와 같음)
v = [0]*4

for i in range(100):
    for s in States:
        v[s] = R[s] + gamma*max([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])
print(v)

optPolicy = [0]*4
for s in States:       
    optPolicy[s] = np.argmax([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])
print(optPolicy) 



# 3.5. Solving the Bellman Optimality Equation


















# 4. Exercise (Gridworld Domain)
P = {
 0: {0: [(0.9,0),(0.1,1),(0,4)], 1: [(0.8,1),(0.1,4),(0.1,0)], 2: [(0.8,4),(0.1,1),(0.1,0)], 3: [(0.9,0),(0.1,4)]},
 1: {0: [(0.8,1),(0.1,2),(0.1,0)], 1: [(0.8,2),(0.2,1)], 2: [(0.8,1),(0.1,0),(0.1,2)], 3: [(0.8,0),(0.2,1)]},
 2: {0: [(0.8,2),(0.1,3),(0.1,1)], 1: [(0.8,3),(0.1,5),(0.1,2)], 2: [(0.8,5),(0.1,1),(0.1,3)], 3: [(0.8,1),(0.1,2),(0.1,5)]},
 3: {0: [(0.9,3),(0.1,2)], 1: [(0.9,3),(0.1,6)], 2: [(0.8,6),(0.1,2),(0.1,3)], 3: [(0.8,2),(0.1,3),(0.1,6)]},
 4: {0: [(0.8,0),(0.2,4)], 1: [(0.8,4),(0.1,7),(0.1,0)], 2: [(0.8,7),(0.2,4)], 3: [(0.8,4),(0.1,0),(0.1,7)]},
 5: {0: [(0.8,2),(0.1,6),(0.1,5)], 1: [(0.8,6),(0.1,9),(0.1,2)], 2: [(0.8,9),(0.1,5),(0.1,6)], 3: [(0.8,5),(0.1,2),(0.1,9)]},
 6: {0: [(0.8,3),(0.1,6),(0.1,5)], 1: [(0.8,6),(0.1,10),(0.1,3)], 2: [(0.8,10),(0.1,5),(0.1,6)], 3: [(0.8,5),(0.1,3),(0.1,10)]},
 7: {0: [(0.8,4),(0.1,8),(0.1,7)], 1: [(0.8,8),(0.1,7),(0.1,4)], 2: [(0.9,7),(0.1,8)], 3: [(0.9,7),(0.1,4)]},
 8: {0: [(0.8,8),(0.1,9),(0.1,7)], 1: [(0.8,9),(0.2,8)], 2: [(0.8,8),(0.1,7),(0.1,9)], 3: [(0.8,7),(0.2,8)]},
 9: {0: [(0.8,5),(0.1,10),(0.1,8)], 1: [(0.8,9),(0.1,9),(0.1,5)], 2: [(0.8,9),(0.1,8),(0.1,10)], 3: [(0.8,8),(0.1,5),(0.1,9)]},
 10: {0: [(0.8,6),(0.1,10),(0.1,9)], 1: [(0.9,10),(0.1,6)], 2: [(0.9,10),(0.1,9)], 3: [(0.8,9),(0.1,6),(0.1,10)]}
}

R = [0, 0, 0, 1, 0, 0, -100, 0, 0, 0, 0]
gamma = 0.9

States = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Actions = [0, 1, 2, 3] # [north, east, south, west]

v = [0] * 11
v_next = [0] * 11


# optimal value fuction
for i in range(100):
    for s in States:
        q_0 = sum(trans[0]*v[trans[1]] for trans in P[s][0])
        q_1 = sum(trans[0]*v[trans[1]] for trans in P[s][1])
        q_2 = sum(trans[0]*v[trans[1]] for trans in P[s][2])
        q_3 = sum(trans[0]*v[trans[1]] for trans in P[s][3])

        v_next[s] = R[s] + gamma*max(q_0, q_1, q_2, q_3)
    v = v_next.copy()
# print(v)


# optimal policy
# once v computed
optPolicy = [0]*11

for s in States:       
    optPolicy[s] = np.argmax([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])

print(optPolicy)















# Example 1 --------------------------------------------------------------------------------------------

import numpy as np

P = {
    0: {0: [(1,1)],
        1: [(1/3,0),(2/3,2)]},
    1: {0: [(1,2)],
        1: [(1/6,0),(1/6,1),(2/3,3)]},
    2: {0: [(1,3)],
        1: [(1/6,1),(1/6,2),(2/3,4)]},
    3: {0: [(1,4)],
        1: [(1/6,2),(1/6,3),(2/3,5)]},
    4: {0: [(1,5)],
        1: [(1/6,3),(1/6,4),(2/3,6)]},
    5: {0: [(1,6)],
        1: [(1/6,4),(1/6,5),(2/3,6)]},
    6: {0: [(1,6)],
        1: [(1/6,5),(5/6,6)]},
}

R = [0, 0, -1, -2, 0, -1, 1]

gamma = 0.6

States = [0,1,2,3,4,5,6]
Actions = [0,1]             # W or J


v = [0] * 7
v_next = [0] * 7

for i in range(100):
    for s in States:
        v[s] = R[s] + gamma*max([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])  
    v = v_next.copy()
print(v)




# optimal policy
optPolicy = [0]*7

for s in States:       
    optPolicy[s] = np.argmax([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])

print(optPolicy)




# 마지막 State에서 돌지 않게
P = {
    0: {0: [(1,1)],
        1: [(1/3,0),(2/3,2)]},
    1: {0: [(1,2)],
        1: [(1/6,0),(1/6,1),(2/3,3)]},
    2: {0: [(1,3)],
        1: [(1/6,1),(1/6,2),(2/3,4)]},
    3: {0: [(1,4)],
        1: [(1/6,2),(1/6,3),(2/3,5)]},
    4: {0: [(1,5)],
        1: [(1/6,3),(1/6,4),(2/3,6)]},
    5: {0: [(1,6)],
        1: [(1/6,4),(1/6,5),(2/3,6)]},
    6: {0: [(1,7)],
        1: [(1,7)]},
    7: {0: [(1,7)],     # 임의로 state 7을 만들경우
        1: [(1,7)]},
}

R = [0, 0, -1, -2, 0, -1, 1, 0]

gamma = 0.6

States = [0,1,2,3,4,5,6,7]
Actions = [0,1] 


v = [0] * 8
v_next = [0] * 8

for i in range(100):
    for s in States:
        v_next[s] = R[s] + gamma*max([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])  
    v = v_next.copy()
print(v)


# optimal policy

optPolicy = [0]*8

for s in States:       
    optPolicy[s] = np.argmax([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])

print(optPolicy)    






# Example 2 --------------------------------------------------------------------------------------------
# P = {
#     0: {0: [(1/2,1), (1/2,5)],
#         1: [(1/2,1), (1/2,5)]},
#     1: {0: [(1,2)],
#         1: [(1,6)]},
#     2: {0: [(1,3)],
#         1: [(1,3)]},
#     3: {0: [(1,3)],
#         1: [(1,3)]},
#     4: {0: [(1,5)],
#         1: [(1,5)]},
#     5: {0: [(1,0)],
#         1: [(1,4)]},
#     6: {0: [(1/2,1), (1/2,5)],
#         1: [(1/2,1), (1/2,5)]}
# }
# R = [2, 0, -10, 0, 1, 0, 1]

# gamma = 0.9

# States = [0, 1, 2, 3, 4, 5, 6]
# Actions = [0, 1] # Fast, Slow



States = [0, 1, 2]  # cool, warm, overheat
Actions = [0, 1] # Fast, Slow

P = {
    0: {0: [(1,0)],
        1: [(1/2, 0), (1/2, 1)]},
    1: {0: [(1/2, 0), (1/2, 1)],
        1: [(1, 2)]},
    2: {0: [(1,2)],
        1: [(1,2)]}
}

R = [1, 2, -10]

gamma = 0.9



v = [0] * 3
v_next = [0] * 3

for i in range(100):
    for s in States:
        v_next[s] = R[s] + gamma*max([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])  
    v = v_next.copy()
print(v)



# optimal policy
optPolicy = [0]*3

for s in States:       
    optPolicy[s] = np.argmax([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])

print(optPolicy) 


































