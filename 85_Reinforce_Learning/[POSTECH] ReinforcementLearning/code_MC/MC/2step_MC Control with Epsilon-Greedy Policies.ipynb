{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.blackjack import BlackjackEnv\n",
    "from lib import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q를 알 때, epsilon-greedy policy함수를 만들어내보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    #빈칸1_Input Q를 maximize하는 action을 얻어내서 채워보자\n",
    "    #단, 실습pdf의 빨간 두번째 빈칸식에 나와있는 epsilon-greedy policy로 만들기\n",
    "    def policy_fn(observation):\n",
    "        \n",
    "        \n",
    "        \n",
    "        return A #A는 각 action별 확률(action개수만큼의 길이를 가짐)\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    #우리는 MC방법을 이용해서 action-value function Q를 구할것이다.\n",
    "    # How : 몇 번의 episode를 시행시켜보고, 그 episode들의 G(return)의 평균을 구해서\n",
    "    # 그 평균값으로 Q를 대체하자. 이를 위해 return G의 총 합과 횟수를 기록하자\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    #즉 쉽게 말하면, Q는 state x action 크기의 행렬이라고 생각하자\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    #우리는 위에서 만든 epsilon_greedy_policy를 이용할 것이다\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    #각각의 에피소드마다\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # 빈칸_2_Generate an episode\n",
    "        #('1step_MC Prediction'의 빈칸_1 과 거의 비슷, 단 policy만 epsilon_greedy_policy확률에 맞춰서 action 선택)\n",
    "        #주의: 현재 policy는 각 action별 확률을 나타냄\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #sa_in_episode에 위의 episode array에서 방문한 state와 action들만 (state, action) pair를 저장하자\n",
    "        sa_in_episode = [(x[0], x[1]) for x in episode]\n",
    "        #각각의 방문했던 (state,action) 마다\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            #밑의빈칸3&4는 실습pdf의 첫번째 빨간 빈칸식 참고\n",
    "            # 빈칸_3_episode중에 처음으로 그 state에 방문하고 action을 취한 idx를 first_occurence_idx에 저장하자.\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # 빈칸_4_처음 방문한 이후로 reward들을 G에 합하자\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # 계산된 G를 해당 state의 returns_sum에 합하고, returns_count까지 이용하여 평균을 Q에 대입하자\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            #빈칸5_결국 우리가 구하고자 하는 action-value function Q에는 어떤값을 대입??\n",
    "            ###Q[state][action] = 으로 시작\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 state별 value funtion 값 plot해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "plotting.plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 우리가 얻어낸 최적의 Policy로 0step에서 했던 실제 블랙잭을 해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_observation(observation):\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    print(\"Player Score: {} (Usable Ace: {}), Dealer Score: {}\".format(\n",
    "          score, usable_ace, dealer_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "win=0\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        print_observation(observation)\n",
    "        action=np.argmax(policy(observation))\n",
    "        print(\"Taking action: {}\".format( [\"Stick\", \"Hit\"][action])) #If action=0, stick. Else action=1, Hit.\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            print_observation(observation)\n",
    "            print(\"Game end. Reward: {}\\n\".format(float(reward)))\n",
    "            if reward==1:\n",
    "                win+=1\n",
    "            break\n",
    "print(\"we win total \",win,\"times!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
