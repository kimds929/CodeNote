{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device : cuda\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "#env = gym.envs.make(\"Breakout-v0\")\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "# device를 설정합니다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device : {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari breakout은 4가지 action으로 작도합니다.\n",
    "# 0 (no action)\n",
    "# 1 (fire)\n",
    "# 2 (left)\n",
    "# 3 (right)\n",
    "# 필요한 parameter들을 설정해줍니다.\n",
    "VALID_ACTIONS = [0, 1, 2, 3]\n",
    "num_episodes = 20000\n",
    "BATCH_SIZE=32\n",
    "GAMMA=0.99\n",
    "TARGET_UPDATE=10000\n",
    "epsilon_start=1\n",
    "epsilon_end=0.1\n",
    "epsilon_decay_steps=1000000\n",
    "LR = 2.5e-4\n",
    "K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "# 학습을 위한 데이터를 저장할 Memory class를 생성합니다.\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        (state, action, nextstate, reward)를 받아서\n",
    "        메모리에 추가하는 함수입니다.\n",
    "        memory가 가득차지 않았다면 tuple을 memory에 추가를하고\n",
    "        가득찼다면 가장 오래된 메모리를 지우고 그 자리에\n",
    "        새로운 tuple을 추가합니다.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        memory에서 batch_size만큼 sampling을 합니다.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape:\n",
    " $$\\text{- Input : }(N, C_{in}, H_{in}, W_{in})$$\n",
    " $$\\text{- Output : }(N, C_{out}, H_{out}, W_{out})$$\n",
    "\n",
    "  $$H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
    "            \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$$\n",
    "  $$W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
    "            \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network\n",
    "    State(image)를 처리하는 CNN network입니다.\n",
    "    State는 이전 frame 4개로 이루어져있습니다. 즉 input_channel=4입니다.\n",
    "    3개의 convolutional layer와 3개의 linear layer로 이루어진 Network입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, h, w, outputs):\n",
    "        \"\"\"\n",
    "        Network에 필요한 layer들을 선언합니다.\n",
    "        필요한 변수들도 받습니다.\n",
    "        h : state image의 높이\n",
    "        w : state image의 넓이\n",
    "        outputs : action의 개수\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(w,kernel_size=8, stride=4), kernel_size=4, stride=2)\n",
    "        convw = conv2d_size_out(convw, kernel_size=3, stride=1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(h,kernel_size=8, stride=4), kernel_size=4, stride=2)\n",
    "        convh = conv2d_size_out(convh, kernel_size=3, stride=1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        self.linear1 = nn.Linear(linear_input_size, 512)\n",
    "        self.linear2 = nn.Linear(512, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        self.__init__에서 생성한 layer들을 사용하여\n",
    "        input x가 들어왔을때 실행되는 계산을\n",
    "        실행합니다.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state를 전처리하는 코드입니다.\n",
    "def rgb2gray(state):\n",
    "    return cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "def resize(state):\n",
    "    return cv2.resize(state, (84, 110))\n",
    "\n",
    "def crop_box(state):\n",
    "    return state[17:17+84,:]\n",
    "\n",
    "def preprocess(state):\n",
    "    out = crop_box(resize(rgb2gray(state)))/255\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy policy 입니다.\n",
    "def select_action(state, eps):\n",
    "    \"\"\"\n",
    "    state : torch tensor shape of (B, C, W, H)\n",
    "    eps : epsilon for epsilon greedy policy\n",
    "    \"\"\"\n",
    "    if random.random()<eps:\n",
    "        return random.choice(VALID_ACTIONS)\n",
    "    else:\n",
    "        policy_net.eval()\n",
    "        with torch.no_grad():\n",
    "            action_prob = policy_net(state).squeeze()\n",
    "        return torch.argmax(action_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 policy_net과 학습과정에서 target을 생성하기 위한 target_net을 만듭니다.\n",
    "init_state = env.reset()\n",
    "init_state = preprocess(init_state)\n",
    "screen_height, screen_width = init_state.shape\n",
    "n_actions = len(VALID_ACTION)\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "# target_net의 weight들을 train_net의 weight와 똑같이 만들어줍니다.\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "# target_net은 학습이 되지 않기 때문에 evaluation mode로 전환합니다.\n",
    "target_net.eval()\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR, momentum=0.95, alpha=0.95, eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e85cc257df443cea09180dbb4746d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# memory를 만들고 capacity의 10%를 랜덤한 state들로 채워줍니다.\n",
    "# 모델에서 사용할 state는 하나의 frame이 아니라 최근 4개의 프레임을 사용한다는 것에 주의합니다.\n",
    "memory = ReplayMemory(120000)\n",
    "state = env.reset()\n",
    "state = preprocess(state)\n",
    "state = np.expand_dims(state, 0)\n",
    "for i in range(K-1):\n",
    "    action = random.choice(VALID_ACTIONS)\n",
    "    next_state, _, _, _ = env.step(action)\n",
    "    next_state = preprocess(next_state)\n",
    "    next_state = np.expand_dims(next_state, 0)\n",
    "    state = np.concatenate([state, next_state], axis=0)\n",
    "state = np.expand_dims(state,0)\n",
    "\n",
    "for i in tqdm_notebook(range(int(memory.capacity/10))):\n",
    "    action = select_action(state, eps=1)\n",
    "    next_state, reward, done, _  = env.step(action)\n",
    "    next_state = preprocess(next_state)\n",
    "    next_state = np.expand_dims(next_state, 0)\n",
    "    next_state = np.expand_dims(next_state, 0)\n",
    "    next_state = np.concatenate([state[:,1:,:,:], next_state], axis=1)\n",
    "    \n",
    "    if done:\n",
    "        next_state = None\n",
    "    reward = np.sign(reward)\n",
    "    \n",
    "    memory.push(torch.tensor(state).float().to(device), \n",
    "                torch.tensor([action]).to(device), \n",
    "                next_state, \n",
    "                torch.tensor([reward]).float().to(device))\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "        state = np.expand_dims(state, 0)\n",
    "        for i in range(K-1):\n",
    "            action = random.choice(VALID_ACTIONS)\n",
    "            next_state, _, _, _ = env.step(action)\n",
    "            next_state = preprocess(next_state)\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "            state = np.concatenate([state, next_state], axis=0)\n",
    "        state = np.expand_dims(state,0)\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 영상으로 보기위한 rendering 함수입니다.\n",
    "def get_render(frames):\n",
    "    fig, ax = plt.subplots()\n",
    "    line, = ax.plot([],[])\n",
    "    def init():\n",
    "        line.set_data([],[])\n",
    "        return (line, )\n",
    "    \n",
    "    def animate(i):\n",
    "        ax.clear()\n",
    "        ax.imshow(frames[i], 'gray', clim=(0,1))\n",
    "        line, = ax.plot([], [])\n",
    "        return (line,)\n",
    "    anim = animation.FuncAnimation(fig, animate, frames = len(frames), interval=30, blit=True)\n",
    "    rc('animation', html='html5')\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    memory에서 batch 만큼의 sample을 뽑아와서 학습을 진행합니다.\n",
    "    \"\"\"\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions)) # batch.state : 길이가 batch_size인 tuple\n",
    "\n",
    "    # Sampling한 sample 각각이 Terminate state에 도달 하였는지 확인해야합니다.\n",
    "    # Terminate state에서는 reward의 기댓값이 0이기 때문에 target policy의\n",
    "    # 값 대신 0으로 업데이트를 진행하여야 하기 때문입니다.\n",
    "    # Terminate state는 None으로 주어질 것입니다.\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = np.stack([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    non_final_next_states = torch.tensor(non_final_next_states).squeeze().float().to(device)\n",
    "    \n",
    "    state_batch = torch.cat(batch.state).squeeze().float().to(device)\n",
    "    action_batch = torch.cat(batch.action).squeeze().to(device)\n",
    "    reward_batch = torch.cat(batch.reward).squeeze().float().to(device)\n",
    "    \n",
    "    # policy_net을 사용하여 state_action_value를 target_net을 사용하여 next_state_value(target_value를\n",
    "    # 계산합니다.\n",
    "    policy_net.train()\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    # Error를 계산합니다.\n",
    "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 학습을 진행합니다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3573d2457a5e448ba0e1bdf4663c847b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Target Network : 2.0, Max Reward : 8.0\n",
      "Update Target Networkd : 1.0, Max Reward : 8.0\n",
      "15634th eposide reward : 2.0, Max Reward : 8.0\r"
     ]
    }
   ],
   "source": [
    "epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "update_count = 0\n",
    "eps_count = 0\n",
    "reward_list = [0]\n",
    "best_episode = None\n",
    "for i_episode in tqdm_notebook(range(num_episodes)):\n",
    "    # 환경을 initialize 합니다.\n",
    "    state_episode = []\n",
    "    state = env.reset()\n",
    "    state_episode.append(state)\n",
    "    state = preprocess(state)\n",
    "    state = np.expand_dims(state, 0)\n",
    "    # K개의 첫 state를 만들어서 합쳐줍니다.\n",
    "    for i in range(K-1):\n",
    "        action = random.choice(VALID_ACTIONS)\n",
    "        next_state, _, _, _ = env.step(action)\n",
    "        next_state = preprocess(next_state)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        state = np.concatenate([state, next_state], axis=0)\n",
    "    state = np.expand_dims(state,0)\n",
    "    REWARD = 0\n",
    "    # for문 안에서 episode의 한스텝을 진행합니다.\n",
    "    # 이전 state S = (s1, s2, s3, s4)라고 한다면 다음 state S`= (s2, s3, s4, s5)가 됩니다.\n",
    "    # 새로운 sample (S, action, S`, reward) 를 메모리에 추가시켜줍니다.\n",
    "    # optimize_model 함수를 사용하여 한번의 update를 진행합니다.\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        if eps_count<len(epsilons):\n",
    "            epsilon = epsilons[eps_count]\n",
    "            eps_count+=1\n",
    "        else:\n",
    "            epsilon = epsilon_end\n",
    "        \n",
    "        action = select_action(state, eps=1)\n",
    "        next_state, reward, done, _  = env.step(action)\n",
    "        next_state = preprocess(next_state)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        next_state = np.concatenate([state[:,1:,:,:], next_state], axis=1)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        reward = np.sign(reward)\n",
    "        memory.push(torch.tensor(state).float().to(device), \n",
    "                    torch.tensor([action]).to(device), \n",
    "                    next_state, \n",
    "                    torch.tensor([reward]).float().to(device))\n",
    "        REWARD += reward\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        update_count+=1\n",
    "        if done:\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if update_count % TARGET_UPDATE == 0:\n",
    "        print('Update Target Network')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    reward_list.append(REWARD)\n",
    "    print('{}th eposide reward : {}, Max Reward : {}'.format(i_episode+1, REWARD, max(reward_list)), end='\\r')\n",
    "    if max(reward_list)==REWARD:\n",
    "        best_episode=state_episode\n",
    "\n",
    "print('Complete')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = preprocess(state)\n",
    "state = np.stack([state] * 4, axis=0)\n",
    "state = torch.from_numpy(state).unsqueeze(0).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.eval()\n",
    "frame_list = []\n",
    "for t in count():\n",
    "    action = select_action(state, eps=0.05)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(reward)\n",
    "    frame_list.append(next_state)\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    next_state = preprocess(next_state)\n",
    "    next_state = torch.from_numpy(next_state).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    next_state = torch.cat([state[:,1:,:,:], next_state], dim=1)\n",
    "        \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[frame for i, frame in enumerate(frame_list) if i%4==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_render([frame for i, frame in enumerate(frame_list) if i%4==0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posco",
   "language": "python",
   "name": "posco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
