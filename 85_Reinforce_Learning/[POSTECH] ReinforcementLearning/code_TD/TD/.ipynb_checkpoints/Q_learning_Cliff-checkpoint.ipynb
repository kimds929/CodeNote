{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    \n",
    "    \n",
    "    #The final action-value function\n",
    "    ## 각자 사용할 Q table을 정의해 줍시다.\n",
    "    # 예시 : A nested dictionary that maps state -> (action-> action-value)\n",
    "    ## 혹은 simply 2d numpy array\n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    ## stats 변수를 정의하고, 여기에 매 episode 길이와 episode reward를 기록합시다.\n",
    "    \n",
    "\n",
    "    \n",
    "    # 매 Episode 마다 ..\n",
    "      ## initialize가 필요합니다.\n",
    "      ## 1. environment를 reset해주고 initial state를 얻습니다.\n",
    "        # The policy we're following\n",
    "        ## epsilon-greedy policy from Q로부터 policy를 얻습니다.\n",
    "      ## 2. 이를 앞서 정의한 policy에 넣어 probability vectgor를 얻고,\n",
    "      ## 3. 위에서 얻은 probability vector에 기반하여 action을 sampling합니다. REFER: np.random.choice\n",
    "    \n",
    "      # 매 time step마다..\n",
    "        # a. 주어진 action으로 step을 진행하고,\n",
    "        # b. 위의 initialize한 과정과 비슷하게 앞서 정의한 policy에 넣어 probability vector를 얻은 뒤\n",
    "        # c. next_action 및 next_state를 얻습니다.\n",
    "        # d. stats 변수에 episode reward와 episode length에 관련된 사항들을 기록합시다.\n",
    "        # e. TD error를 계산하고, Q-learning update 식을 토대로 Q를 update해 줍시다.\n",
    "        # f. 만약 현재 environment가 terminal이라면, for문을 break해 주고,\n",
    "        # g. 아니라면 action과 state를 update해줍시다.\n",
    "     \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, stats = q_learning(env, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_stats(stats, smoothing_window=10, noshow=False):\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(stats.episode_lengths)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Length\")\n",
    "    plt.title(\"Episode Length over Time\")\n",
    "    if noshow:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "\n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=(10,5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    if noshow:\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show(fig2)\n",
    "\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    if noshow:\n",
    "        plt.close(fig3)\n",
    "    else:\n",
    "        plt.show(fig3)\n",
    "\n",
    "    return fig1, fig2, fig3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
